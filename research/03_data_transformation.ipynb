{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875de2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\fiscarelli\\\\Desktop\\\\Progetti\\\\Manpower IT\\\\Auto-Match\\\\Candidates-to-Jobs-Auto-Match-Cortex-AI'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"snowflake\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"snowflake.snowpark\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "\n",
    "# Add the absolute path to src/ so Python can find automatch\n",
    "src_path = os.path.abspath(\"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c1705f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\fiscarelli\\\\Desktop\\\\Progetti\\\\Manpower IT\\\\Auto-Match\\\\Candidates-to-Jobs-Auto-Match-Cortex-AI'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9155a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n",
      "Going to open: https://login.microsoftonline.com/e2ba81b8-03fe-407c-96a1-f4bc0f512e7d/saml2?SAMLRequest=nZJNc9owEIb%2Fikc925IM4UMDZAiUlk5CPMFkptxkew0ituRIcgz99RUmzKSH5NCbR3529WjfHd0ey8J7A22EkmNEA4I8kKnKhNyN0SZe%2BAPkGctlxgslYYxOYNDtZGR4WVRsWtu9fILXGoz1XCNpWPtjjGotmeJGGCZ5CYbZlK2nD%2FcsDAirtLIqVQX6UPJ1BTcGtHWG15LMCKe3t7ZiGDdNEzSdQOkdDgkhmAyxo87Ityt%2FdG%2F6hKeYdM%2B8IxwevbvdCXkZwVdayQUy7GccR370uI6RN72qzpQ0dQl6DfpNpLB5ur8IGGdwtx52STgIGjc3H2qtKgj4n1pDYKRq8oK%2FQKrKqraue%2BC%2BcA4ZLtROuAEs52NUvYjs12G1fa2i47Ysj7P982HXT5LD4yD%2BPYx%2F8O8bFXG6yMlp9TBbpsh7viYcnhNeGlPDUp5zte6IhDc%2BJX5IY0oY6bEOCWivv0Xe3PkJyW1beZVvPYJSpFoZlVslCyGhtYQw4QOaDHzSycHvkn7qD3uc%2Bnk3SUl%2BQ0PoZ%2FicdoguG8RaET3537mM8Mcu70u5cjkt55EqRHryFkqX3H4eIw1oeyIyP29RBiUXxTTLNBjj4iwK1cw0cOt23%2BoaEJ5cbv13%2Byd%2FAQ%3D%3D&RelayState=ver%3A1-hint%3A126482533224670-ETMsDgAAAZoGPA0XABRBRVMvQ0JDL1BLQ1M1UGFkZGluZwEAABAAEKN4Gx7L461xVxhRVrnPnoEAAACgaT1olI0U6kHXLsrFxmMQpyA8w%2F%2FkxULo%2B96GpN3GZbS%2BQqrNmb1br%2FdGzsP9AHG%2B3AGE3Vqi%2BvvolMV%2FSBEhS5asG2f%2Fu8nzJNWDYL%2BNN2zMvp7uOpT2rcnHlC6TFzLnopA%2BKfMZ%2BbRVEspiIrEFWFinnpNpsECZniaDzg1QG362i8A6zxpqlrTMAAPh%2FbGGjHDqYotn1aJ1lLW56Smp6gAUaH2lD8oZCoa3u94SROT9Y2oy0tg%3D to authenticate...\n"
     ]
    }
   ],
   "source": [
    "from autoMatch.utils.snowflake_utils import get_snowpark_session\n",
    "session = get_snowpark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc40000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: str\n",
    "    database: str\n",
    "    schema: str\n",
    "    input_table: str\n",
    "    input_table_cleaned: str\n",
    "    input_table_italian_cities: str\n",
    "    output_table: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdecf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoMatch.constants import *\n",
    "from autoMatch.utils.common import read_yaml, create_directories\n",
    "from autoMatch import logger\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            database=config.database,\n",
    "            schema=config.schema,\n",
    "            input_table=config.input_table,\n",
    "            input_table_cleaned=config.input_table_cleaned,\n",
    "            input_table_italian_cities = config.input_table_italian_cities,\n",
    "            output_table = config.output_table,\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af745682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col, trim, lower, length, parse_json, when, lit, trim, to_date, to_varchar\n",
    "from snowflake.snowpark.types import StringType, BooleanType\n",
    "from snowflake.snowpark.functions import udf\n",
    "\n",
    "from datetime import dates\n",
    "\n",
    "from autoMatch.utils.common import validate_string\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def clean_description(self, session):\n",
    "        \"\"\"\n",
    "        Reads input table\n",
    "        Cleans description column:\n",
    "            - removes rows with empty description\n",
    "            - replaces multiple consecutive whitespaces with a single whitespace (preserves newlines)\n",
    "            - removes all html tags\n",
    "            - lowercases all text\n",
    "        Function returns Snowflake dataframe\n",
    "        \"\"\"\n",
    "        database = self.config.database\n",
    "        schema = self.config.schema\n",
    "        input_table = self.config.input_table\n",
    "\n",
    "        df = session.table(f\"{database}.{schema}.{input_table}\")\n",
    "        df = df.filter((col(\"description\").is_not_null()) & (trim(col(\"description\")) != \"\"))\n",
    "\n",
    "        def build_normalize_whitespace_udf():\n",
    "            def normalize(text: str) -> str:\n",
    "                import re\n",
    "                if text is None:\n",
    "                    return ''\n",
    "                return re.sub(r'[ \\t]+', ' ', text).strip()\n",
    "\n",
    "            return udf(normalize, return_type=StringType(), input_types=[StringType()])\n",
    "        normalize_udf = build_normalize_whitespace_udf()\n",
    "        df = df.with_column(\"description\", normalize_udf(df[\"description\"]))\n",
    "        \n",
    "\n",
    "        def build_clean_html_udf():\n",
    "            from bs4 import BeautifulSoup\n",
    "            def clean_html(text: str) -> str:\n",
    "                if not text:\n",
    "                    return \"\"\n",
    "                return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "            return udf(\n",
    "                clean_html,\n",
    "                return_type=StringType(),\n",
    "                input_types=[StringType()],\n",
    "                packages=[\"beautifulsoup4\"]\n",
    "            )\n",
    "\n",
    "        clean_html_udf = build_clean_html_udf()\n",
    "        df = df.with_column(\"description\", clean_html_udf(df[\"description\"]))\n",
    "\n",
    "        df = df.with_column(\"description\", lower(df[\"description\"]))\n",
    "\n",
    "        df = df.filter((col(\"description\").is_not_null()) & \n",
    "                       (trim(col(\"description\")) != \"\") &\n",
    "                       (length(trim(col(\"description\"))) > 5) &\n",
    "                       (~col(\"description\").like(\"%None%\")) &\n",
    "                       (~col(\"description\").like(\"%null%\"))\n",
    "                       )\n",
    "\n",
    "        df = df.with_column(\"description\", col(\"description\").cast(\"STRING\"))\n",
    "        \n",
    "        logger.info(f\"Table {input_table} successfully cleaned\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def apply_ner_cortexai(self, session):\n",
    "        \"\"\"\n",
    "        Reads input table cleaned\n",
    "        Performs Named Entity Recognition:\n",
    "            - age\n",
    "            - date_of_birth\n",
    "            - location\n",
    "            - zip_code\n",
    "            - last_job\n",
    "            - second_last_job\n",
    "            - third_last_job\n",
    "            - skills\n",
    "        Function returns Snowflake dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        database = self.config.database\n",
    "        schema = self.config.schema\n",
    "        input_table_cleaned = self.config.input_table_cleaned\n",
    "\n",
    "        today_string = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT\n",
    "                *,\n",
    "                SNOWFLAKE.CORTEX.COMPLETE(\n",
    "                    'claude-4-sonnet',\n",
    "                    CONCAT(\n",
    "                        'Estrai dal seguente testo i campi: \n",
    "                        age (stringa. calcolala basandoti su date_of_birth, considerando che la data di oggi e {today_string};\n",
    "                            esempio: se date_of_birth = 1990-05-18 e oggi siamo in data 2025-09-15, age = 35 \n",
    "                            se non è possibile calcolarla, restituisci NaN), \n",
    "                        date_of_birth (stringa in formato YYYY-MM-DD, \n",
    "                            YYYY deve essere compreso tra 1900 e {today_string},\n",
    "                            MM deve essere compreso tra 1 e 12,\n",
    "                            DD deve essere compreso tra: \n",
    "                                1 e 30 quando MM uguale a 11, 04, 06, 09;\n",
    "                                1 e 28 quando MM uguale a 02\n",
    "                                1 e 31 per i restanti casi\n",
    "                            ), \n",
    "                        location (stringa. solo la citta, non includere province o altro), \n",
    "                        zip_code (cap. 5 caratteri numerici), \n",
    "                        last_job, \n",
    "                        second_last_job, \n",
    "                        third_last_job, \n",
    "                        skills (stringa)',\n",
    "                        'Rispondi in formato JSON, senza testo extra, attieniti a questo esempio: \n",
    "                        {{\"age\": 30, \"date_of_birth\": \"1993-05-12\", \"location\": \"Milano\", \"zip_code\": \"20100\", \"last_job\": \"Data Engineer\", \"second_last_job\": \"Developer\", \"third_last_job\": \"Intern\", \"skills\": \"Python, SQL, Java\"}}. ',\n",
    "                        'Testo: ', description\n",
    "                    )\n",
    "                ) AS ner_json\n",
    "            FROM \n",
    "            (SELECT *\n",
    "            FROM {database}.{schema}.{input_table_cleaned}\n",
    "            )\n",
    "\n",
    "            \"\"\"\n",
    "        df = session.sql(query)\n",
    "\n",
    "        def build_clean_parsing_udf():\n",
    "            def clean(x: str) -> str:\n",
    "                if x is None:\n",
    "                    return ''\n",
    "                x = x.lower().lstrip()\n",
    "                if x.startswith(\"```json\"):\n",
    "                    x = x[8:].lstrip()\n",
    "                x = x.replace('\\n', ' ').replace('\\t', ' ').replace('\\\\', '').strip()\n",
    "                x = ' '.join(x.split())\n",
    "                if x.endswith(\"```\"):\n",
    "                    x = x[:-3].rstrip()\n",
    "                if x.endswith(\"'\") or x.endswith('\"'):\n",
    "                    x = x[:-1].rstrip()\n",
    "                return x\n",
    "\n",
    "            return udf(clean, return_type=StringType(), input_types=[StringType()])\n",
    "        clean_udf = build_clean_parsing_udf()\n",
    "\n",
    "        df = df.with_column(\"ner_json\", clean_udf(df[\"ner_json\"]))\n",
    "\n",
    "        def build_is_valid_json_udf():\n",
    "            import json\n",
    "            def is_valid(text: str) -> bool:\n",
    "                if not text:\n",
    "                    return False\n",
    "                try:\n",
    "                    json.loads(text)\n",
    "                    return True\n",
    "                except Exception:\n",
    "                    return False\n",
    "\n",
    "            return udf(is_valid, return_type=BooleanType(), input_types=[StringType()])\n",
    "        \n",
    "        is_valid_json_udf = build_is_valid_json_udf()\n",
    "        df = df.with_column(\"is_valid_json\", is_valid_json_udf(df[\"ner_json\"]))\n",
    "        df = df.filter(col(\"is_valid_json\") == True)\n",
    "        df = df.drop(\"is_valid_json\")\n",
    "\n",
    "        df = df.with_column(\"ner_json\", parse_json(col(\"ner_json\")))\n",
    "\n",
    "        df = df.filter(df[\"ner_json\"].is_not_null())\n",
    "\n",
    "        df = df.with_columns(\n",
    "            [\"age\", \"date_of_birth\", \"location\", \"zip_code\", \"last_job\", \"second_last_job\", \"third_last_job\", \"skills\"],\n",
    "            [\n",
    "                col(\"ner_json\")[\"age\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"date_of_birth\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"location\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"zip_code\"].cast(\"STRING\"), \n",
    "                col(\"ner_json\")[\"last_job\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"second_last_job\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"third_last_job\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"skills\"].cast(\"STRING\")\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        df = validate_string(df, \"location\")\n",
    "        df = validate_string(df, \"last_job\")\n",
    "        df = validate_string(df, \"second_last_job\")\n",
    "        df = validate_string(df, \"third_last_job\")\n",
    "        df = validate_string(df, \"skills\")\n",
    "\n",
    "        # makes sure zip_code is a valid formato\n",
    "        df = df.with_column(\n",
    "            \"zip_code\",\n",
    "            when(\n",
    "                col(\"zip_code\").rlike(\"^[0-9]{5}$\"),\n",
    "                col(\"zip_code\")\n",
    "            ).otherwise(lit(None))\n",
    "        )\n",
    "\n",
    "        # makes sure age is a reasonable value\n",
    "        df = df.with_column(\n",
    "            \"age\",\n",
    "            when(\n",
    "                (col(\"age\") != \"nan\") &\n",
    "                (col(\"age\").cast(\"INT\").is_not_null()) &\n",
    "                (col(\"age\").cast(\"INT\") >= 1) &\n",
    "                (col(\"age\").cast(\"INT\") <= 150),\n",
    "                col(\"age\").cast(\"INT\")\n",
    "            ).otherwise(lit(None))\n",
    "        )\n",
    "\n",
    "        # makes sure date_of_birth is a valid date\n",
    "        date_regex = (\n",
    "            \"^(\" +\n",
    "            # Months with 31 days\n",
    "            f\"(19[0-9][0-9]|20[0-9][0-9]|{today_string})-(01|03|05|07|08|10|12)-(0[1-9]|[12][0-9]|3[01])|\" +\n",
    "            # Months with 30 days\n",
    "            f\"(19[0-9][0-9]|20[0-9][0-9]|{today_string})-(04|06|09|11)-(0[1-9]|[12][0-9]|30)|\" +\n",
    "            # February (non-leap year logic: 1–28)\n",
    "            f\"(19[0-9][0-9]|20[0-9][0-9]|{today_string})-02-(0[1-9]|1[0-9]|2[0-8])\" +\n",
    "            \")$\"\n",
    "        )\n",
    "        df = df.with_column(\n",
    "            \"date_of_birth\",\n",
    "            when(\n",
    "                col(\"date_of_birth\").rlike(date_regex),\n",
    "                to_date(to_varchar(col(\"date_of_birth\")), \"YYYY-MM-DD\"),\n",
    "            ).otherwise(lit(None))\n",
    "        )\n",
    "        \n",
    "        df = df.drop(\"ner_json\")\n",
    "        df = df.drop(\"description\")\n",
    "\n",
    "        logger.info(f\"NER on {input_table_cleaned} table successful\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def add_geo_info(self, session):\n",
    "        \"\"\"\n",
    "        Reads candidate dataframe\n",
    "        Reads italian cities dataframe\n",
    "        Checks that zip_codes are valid\n",
    "        Adds geo info (latitude, longitude)\n",
    "        Function returns Snowflake dataframe\n",
    "        \"\"\"\n",
    "        database = self.config.database\n",
    "        schema = self.config.schema\n",
    "        output_table = self.config.output_table\n",
    "        input_table_italian_cities = self.config.input_table_italian_cities\n",
    "        \n",
    "        candidates = session.table(f\"{database}.{schema}.{output_table}\")\n",
    "        italian_cities = session.table(f\"{database}.{schema}.{input_table_italian_cities}\")\n",
    "\n",
    "        # This avoids issues when the candidates table has already lat and long columns\n",
    "        candidates = candidates.select(\n",
    "            *[col(c) for c in candidates.columns if c.lower() not in {\"latitude\", \"longitude\", \"province_ext\"}]\n",
    "        )\n",
    "\n",
    "        valid_zips = [\n",
    "            row[key]\n",
    "            for row in italian_cities.select(\"zip\").distinct().collect()\n",
    "            for key in row.as_dict()\n",
    "            if key.lower() == \"zip\"\n",
    "        ]\n",
    "        # Replace invalid ZIPs with None\n",
    "        candidates = candidates.with_column(\n",
    "            \"zip_code\",\n",
    "            when(col(\"zip_code\").isin(valid_zips), col(\"zip_code\")).otherwise(lit(None))\n",
    "            )\n",
    "\n",
    "        # If zip_code is missing and location is available, get the zip_code from italian_cities dataframe\n",
    "        df = candidates.join(\n",
    "            italian_cities,\n",
    "            lower(candidates[\"location\"]) == lower(italian_cities[\"city_name\"]),\n",
    "            \"left\"\n",
    "        )\n",
    "        df = df.with_column(\n",
    "            \"zip_code\",\n",
    "            when(\n",
    "                col(\"zip_code\").is_null(), col(\"zip\") \n",
    "            ).otherwise(col(\"zip_code\"))\n",
    "        )\n",
    "        df = df.select(\n",
    "            *[col(c.name) for c in candidates.schema.fields],\n",
    "            )\n",
    "\n",
    "        # Given the candidate location, get (lat, long) data from italian_cities\n",
    "        df = df.join(\n",
    "            italian_cities,\n",
    "            lower(candidates[\"location\"]) == lower(italian_cities[\"city_name\"]),\n",
    "            how=\"left\"\n",
    "        )\n",
    "        df = df.select(\n",
    "            *[col(c.name) for c in candidates.schema.fields],\n",
    "            col(\"latitude\"),\n",
    "            col(\"longitude\"),\n",
    "            col(\"province_ext\"),\n",
    "            )\n",
    "        \n",
    "        df = validate_string(df, \"province_ext\")\n",
    "\n",
    "        # Regex for latitude: -90 to 90 with optional decimals\n",
    "        latitude_regex = r\"^[-+]?([0-8]?\\d(\\.\\d+)?|90(\\.0+)?)$\"\n",
    "        # Regex for longitude: -180 to 180 with optional decimals\n",
    "        longitude_regex = r\"^[-+]?((1[0-7]\\d|0?\\d{1,2})(\\.\\d+)?|180(\\.0+)?)$\"\n",
    "\n",
    "        # Make sure latitude and longiture are valid\n",
    "        df = df.with_column(\n",
    "            \"latitude\",\n",
    "            when(\n",
    "                col(\"latitude\").rlike(latitude_regex),\n",
    "                col(\"latitude\").cast(\"FLOAT\")\n",
    "            ).otherwise(lit(None))\n",
    "        ).with_column(\n",
    "            \"longitude\",\n",
    "            when(\n",
    "                col(\"longitude\").rlike(longitude_regex),\n",
    "                col(\"longitude\").cast(\"FLOAT\")\n",
    "            ).otherwise(lit(None))\n",
    "        )\n",
    "\n",
    "        #df = df.with_column(\"distance_km\", lit(None))\n",
    "        df = df.with_column(\"distance_km\", lit(999999))\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def write_table(self, df, table_name = 'output_table'):\n",
    "        \"\"\"\n",
    "        Writes table\n",
    "        Function returns nothing\n",
    "        \"\"\"\n",
    "        df.write.save_as_table(table_name, mode=\"overwrite\")\n",
    "        logger.info(f\"Table {table_name} successfully written\")\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d69f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-21 12:06:37,015: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-10-21 12:06:37,018: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-10-21 12:06:37,046: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-10-21 12:06:37,049: INFO: common: created directory at: artifacts]\n",
      "[2025-10-21 12:06:37,053: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-10-21 12:06:41,980: INFO: 1758941708: Table MPG_IT_AUTOMATCH_CANDIDATE_FEATURES successfully written]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    #df = data_transformation.clean_description(session)\n",
    "    #data_transformation.write_table(df, data_transformation.config.input_table_cleaned)\n",
    "    #df = data_transformation.apply_ner_cortexai(session)\n",
    "    #data_transformation.write_table(df, data_transformation.config.output_table)\n",
    "    df = data_transformation.add_geo_info(session)\n",
    "    data_transformation.write_table(df, data_transformation.config.output_table)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9768f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea37560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d425155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE_ADDED</th>\n",
       "      <th>CANDIDATEID</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>LAST_JOB</th>\n",
       "      <th>SECOND_LAST_JOB</th>\n",
       "      <th>THIRD_LAST_JOB</th>\n",
       "      <th>SKILLS</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DATE_OF_BIRTH</th>\n",
       "      <th>ZIP_CODE</th>\n",
       "      <th>PROVINCE_EXT</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>DISTANCE_KM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-16 02:40:29.727</td>\n",
       "      <td>5546247</td>\n",
       "      <td>cernusco sul naviglio</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>web developer base (html, css, javascript, php...</td>\n",
       "      <td>19</td>\n",
       "      <td>2006-09-20</td>\n",
       "      <td>20063</td>\n",
       "      <td>Milano</td>\n",
       "      <td>45.524558</td>\n",
       "      <td>9.330925</td>\n",
       "      <td>999999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               DATE_ADDED  CANDIDATEID               LOCATION LAST_JOB  \\\n",
       "0 2025-07-16 02:40:29.727      5546247  cernusco sul naviglio     None   \n",
       "\n",
       "  SECOND_LAST_JOB THIRD_LAST_JOB  \\\n",
       "0            None           None   \n",
       "\n",
       "                                              SKILLS  AGE DATE_OF_BIRTH  \\\n",
       "0  web developer base (html, css, javascript, php...   19    2006-09-20   \n",
       "\n",
       "  ZIP_CODE PROVINCE_EXT   LATITUDE  LONGITUDE  DISTANCE_KM  \n",
       "0    20063       Milano  45.524558   9.330925       999999  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM IT_DISCOVERY.CONSUMER_INT_MODEL.MPG_IT_AUTOMATCH_CANDIDATE_FEATURES\n",
    "    WHERE candidateid='5546247'\n",
    "\"\"\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01aeb555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "|\"NER_JSON\"                                          |\n",
      "------------------------------------------------------\n",
      "|```json                                             |\n",
      "|{\"age\": \"30\", \"date_of_birth\": \"1993-05-15\", \"l...  |\n",
      "|```                                                 |\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark import Session\n",
    "\n",
    "# Example description string\n",
    "description_text = \"\"\"\n",
    "Mario Rossi, nata a vorbomanero (bo) il 15051993. CAP 20100.\n",
    "Ha lavorato come Data Engineer, prima come Developer e ancora prima come Intern.\n",
    "Competenze: Python, SQL, Java.\n",
    "\"\"\"\n",
    "\n",
    "# Construct the SQL query\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    SNOWFLAKE.CORTEX.COMPLETE(\n",
    "        'claude-4-sonnet',\n",
    "        CONCAT(\n",
    "            'Estrai dal seguente testo i campi: \n",
    "            age (stringa. caratteri numerici), \n",
    "            date_of_birth (YYYY-MM-DD), \n",
    "            location (stringa. solo la citta, non includere province o altro), \n",
    "            zip_code (cap. 5 caratteri numerici), \n",
    "            last_job, \n",
    "            second_last_job, \n",
    "            third_last_job, \n",
    "            skills (stringa)',\n",
    "            'Rispondi in formato JSON, senza testo extra, attieniti a questo esempio: \n",
    "            {{\"age\": 30, \"date_of_birth\": \"1993-05-12\", \"location\": \"Milano\", \"zip_code\": \"20100\", \"last_job\": \"Data Engineer\", \"second_last_job\": \"Developer\", \"third_last_job\": \"Intern\", \"skills\": \"Python, SQL, Java\"}}. ',\n",
    "            'Testo: ', '{description_text}'\n",
    "        )\n",
    "    ) AS ner_json\n",
    "\"\"\"\n",
    "\n",
    "# Run the query\n",
    "result_df = session.sql(query)\n",
    "\n",
    "# Show the result\n",
    "result_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
