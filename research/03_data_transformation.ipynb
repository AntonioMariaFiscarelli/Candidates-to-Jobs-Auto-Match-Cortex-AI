{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875de2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\fiscarelli\\\\Desktop\\\\Progetti\\\\Manpower IT\\\\Auto-Match\\\\Candidates-to-Jobs-Auto-Match-Cortex-AI'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"snowflake\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"snowflake.snowpark\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "\n",
    "# Add the absolute path to src/ so Python can find automatch\n",
    "src_path = os.path.abspath(\"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c1705f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\fiscarelli\\\\Desktop\\\\Progetti\\\\Manpower IT\\\\Auto-Match\\\\Candidates-to-Jobs-Auto-Match-Cortex-AI'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9155a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n",
      "Going to open: https://login.microsoftonline.com/e2ba81b8-03fe-407c-96a1-f4bc0f512e7d/saml2?SAMLRequest=nZJbb%2BIwEIX%2FSuR9TuIYuoAFVEChi9QLKmml7ZtJJmDVsVOP09D%2B%2BjpcpO5D%2B7Bvln1mvuM5M7zclyp4A4vS6BFJIkoC0JnJpd6OyGO6CPskQCd0LpTRMCLvgORyPERRqopParfTD%2FBaA7rAN9LI24cRqa3mRqBErkUJyF3G15PbG84iygUiWOdx5FSSo%2FSsnXMVj%2BOmaaKmExm7jRmlNKaD2KtayS%2FyBVH9zKiscSYz6lyy93%2F6BpHEtNsivMITVqfCqdTHEfxE2RxFyP%2Bk6Spc3a9TEkzOv5sZjXUJdg32TWbw%2BHBzNIDewXQ96FLWjxo%2FtxBqayqIxEdtIUJtmkKJF8hMWdXOd4%2F8KS4gj5XZSj%2Bz5dWIVC8yVx%2BL%2FfU9lrfL17mbd%2F66efp8lz5Ni0JukJYpXO96YjZZsO48I8HTOWHWJrxErGGp21ydv6LsIkxoSHspHXBGeacTsR57JsGV9ye1cIfKs%2FmDj6iUmTVoCme0khoOLoFtRD%2FZ9EPaKSDs0l4WDn6LJCy6m4wWFwmDXh636TFy3CB%2BMGLH%2FzuXYfy1y2kp73xOy6uVUTJ7DxbGlsJ9H2MSJYcbmYfFQcqhFFJN8twCoo9TKdPMLAjnd9%2FZGkg8PlL%2F3f7xJw%3D%3D&RelayState=ver%3A1-hint%3A126482531967198-ETMsDgAAAZm9%2BPP4ABRBRVMvQ0JDL1BLQ1M1UGFkZGluZwEAABAAEDjO20RiyPfWAM8SeK8qA4oAAACg%2F9DWqpMwD7GFdOS5v2Su7x1u1rgsjpSejCErrv9fvGar%2F98R3Y%2FWZ2a5wFgAP%2F7hI7160NCa4XQf1hx84aWnGCYfuh379mRjq%2F8CUpLDUoY391np2NJci9vN6QGlr1SnOgOqIKIt5L4TzKD34x8itu%2BjosxKZCgZJAhLidFQUwQZpxaf1JYTOeMMy6K0r%2B66dLZKh0qhpEz6fNZsUiUw2wAUPFTPMyf3zDeD8WOVK9ejknFLbB4%3D to authenticate...\n"
     ]
    }
   ],
   "source": [
    "from autoMatch.utils.snowflake_utils import get_snowpark_session\n",
    "session = get_snowpark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc40000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: str\n",
    "    database: str\n",
    "    schema: str\n",
    "    input_table: str\n",
    "    input_table_cleaned: str\n",
    "    output_table: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdecf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoMatch.constants import *\n",
    "from autoMatch.utils.common import read_yaml, create_directories\n",
    "from autoMatch import logger\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            database=config.database,\n",
    "            schema=config.schema,\n",
    "            input_table=config.input_table,\n",
    "            input_table_cleaned=config.input_table_cleaned,\n",
    "            output_table = config.output_table,\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af745682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col, trim, lower, length, expr\n",
    "from snowflake.snowpark.types import StringType\n",
    "\n",
    "from snowflake.snowpark.functions import udf\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def clean_description(self, session):\n",
    "        \"\"\"\n",
    "        Reads input table\n",
    "        Cleans description column:\n",
    "            - removes rows with empty description\n",
    "            - replaces multiple consecutive whitespaces with a single whitespace (preserves newlines)\n",
    "            - removes all html tags\n",
    "            - lowercases all text\n",
    "        Performs Named Entity Recognition\n",
    "        Function returns Snowflake dataframe\n",
    "        \"\"\"\n",
    "        database = self.config.database\n",
    "        schema = self.config.schema\n",
    "        input_table = self.config.input_table\n",
    "        input_table_cleaned = self.config.input_table_cleaned\n",
    "\n",
    "        df = session.table(f\"{database}.{schema}.{input_table}\").limit(100)\n",
    "\n",
    "        df = df.filter((col(\"description\").is_not_null()) & (trim(col(\"description\")) != \"\"))\n",
    "\n",
    "\n",
    "        def normalize_whitespace(text: str) -> str:\n",
    "            import re\n",
    "            return re.sub(r'[ \\t]+', ' ', text).strip()\n",
    "\n",
    "        normalize_whitespace_udf = udf(\n",
    "            normalize_whitespace, \n",
    "            return_type=StringType(), \n",
    "            input_types=[StringType()]\n",
    "            )\n",
    "\n",
    "        df = df.with_column(\"description\", normalize_whitespace_udf(df[\"description\"]))\n",
    "\n",
    "\n",
    "        def clean_html(text: str) -> str:\n",
    "            from bs4 import BeautifulSoup\n",
    "            if not text:\n",
    "                return \"\"\n",
    "            return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "        clean_html_udf = udf(\n",
    "            clean_html,\n",
    "            return_type=StringType(),\n",
    "            input_types=[StringType()],\n",
    "            packages=[\"beautifulsoup4\"]\n",
    "            )\n",
    "\n",
    "        df = df.with_column(\"description\", clean_html_udf(df[\"description\"]))\n",
    "\n",
    "        df = df.with_column(\"description\", lower(df[\"description\"]))\n",
    "\n",
    "        df = df.filter((col(\"description\").is_not_null()) & \n",
    "                       (trim(col(\"description\")) != \"\") &\n",
    "                       (length(trim(col(\"description\"))) > 5) &\n",
    "                       (~col(\"description\").like(\"%None%\")) &\n",
    "                       (~col(\"description\").like(\"%null%\"))\n",
    "                       )\n",
    "\n",
    "        df = df.with_column(\"description\", col(\"description\").cast(\"STRING\"))\n",
    "        \n",
    "        def remove_special_chars(text: str) -> str:\n",
    "            import re\n",
    "            if not text:\n",
    "                return \"\"\n",
    "            return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        remove_special_chars_udf = udf(\n",
    "            remove_special_chars,\n",
    "            return_type=StringType(),\n",
    "            input_types=[StringType()]\n",
    "        )\n",
    "        df = df.with_column(\"description\", remove_special_chars_udf(df[\"description\"]))\n",
    "\n",
    "        df.write.save_as_table(\n",
    "            input_table_cleaned,\n",
    "            mode=\"overwrite\",\n",
    "        )\n",
    "        logger.info(f\"Table {input_table} successfully cleaned\")\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT\n",
    "                *,\n",
    "                SNOWFLAKE.CORTEX.COMPLETE(\n",
    "                    'claude-4-sonnet',\n",
    "                    CONCAT(\n",
    "                        'Estrai dal seguente testo i campi: age (numero), date_of_birth (YYYY-MM-DD), location (stringa), zip_code (numero), last_job, second_last_job, third_last_job, skills (stringa). ',\n",
    "                        'Rispondi in formato JSON, senza testo extra, attieniti a questo esempio: {{\"age\": 30, \"date_of_birth\": \"1993-05-12\", \"location\": \"Milano\", \"zip_code\": 20100, \"last_job\": \"Data Engineer\", \"second_last_job\": \"Developer\", \"third_last_job\": \"Intern\", \"skills\": \"Python, SQL\"}}. ',\n",
    "                        'Testo: ', description\n",
    "                    )\n",
    "                ) AS parsed_json\n",
    "            FROM {database}.{schema}.{input_table_cleaned}\n",
    "\n",
    "            \"\"\"\n",
    "         \n",
    "        import json\n",
    "\n",
    "        response_format = {\n",
    "            \"età\": \"Qual è l età del candidato? (numero)\",\n",
    "            \"data_di_nascita\": \"Qual e la data di nascita del candidato? Restituisci solo uno in formato stringa ANNO-MESE-GIORNO\",\n",
    "            \"località\": \"Qual è la località del candidato? Restituisci solo uno in formato stringa\",\n",
    "            \"cap\": \"Qual è il codice postale del candidato? Restituisci solo uno in formato stringa\",\n",
    "            \"ultimo_lavoro\": \"Qual è l ultimo lavoro svolto dal candidato? Restituisci solo uno in formato stringa, il tipo di lavoro, non l azienda o il datore di lavoro\",\n",
    "            \"penultimo_lavoro\": \"Qual è il penultimo lavoro svolto dal candidato? Restituisci solo uno in formato stringa, il tipo di lavoro, non l azienda o il datore di lavoro\",\n",
    "            \"terzultimo_lavoro\": \"Qual è il terzultimo lavoro svolto dal candidato? Restituisci solo uno in formato stringa, il tipo di lavoro, non l azienda o il datore di lavoro\",\n",
    "            \"competenze\": \"Qual è l elenco delle competenze del candidato? (stringa di competenze separate da virgola)\"\n",
    "        }\n",
    "\n",
    "        response_json = json.dumps(response_format)\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT \n",
    "                *, \n",
    "                AI_EXTRACT(\n",
    "                text => TO_VARCHAR(description),\n",
    "                responseFormat => PARSE_JSON('{response_json}')\n",
    "                ) AS ner_json\n",
    "                FROM {database}.{schema}.{input_table_cleaned}\n",
    "        \"\"\"\n",
    "        \n",
    "        qq = f\"\"\"\n",
    "                AI_EXTRACT(\n",
    "                text => description,\n",
    "                responseFormat => PARSE_JSON('{response_json}')\n",
    "                )\n",
    "        \"\"\"\n",
    "        qq2 = \"\"\" \n",
    "                SNOWFLAKE.CORTEX.COMPLETE(\n",
    "                    'claude-4-sonnet',\n",
    "                    CONCAT(\n",
    "                        'Estrai dal seguente testo i campi: age (numero), date_of_birth (YYYY-MM-DD), location (stringa), zip_code (numero), last_job, second_last_job, third_last_job, skills (stringa). ',\n",
    "                        'Rispondi in formato JSON, senza testo extra, attieniti a questo esempio: {{\"age\": 30, \"date_of_birth\": \"1993-05-12\", \"location\": \"Milano\", \"zip_code\": 20100, \"last_job\": \"Data Engineer\", \"second_last_job\": \"Developer\", \"third_last_job\": \"Intern\", \"skills\": \"Python, SQL\"}}. ',\n",
    "                        'Testo: ', description\n",
    "                    )\n",
    "                )\n",
    "                \"\"\"\n",
    "\n",
    "        '''\n",
    "        df = session.sql(f\"SELECT * FROM {database}.{schema}.{input_table_cleaned}\")\n",
    "        descriptions = df.select(col(\"description\").alias(\"description\")).collect()\n",
    "        rcount = ecount = 0\n",
    "        for row in descriptions:\n",
    "            desc = row[0]  # or row[\"description\"] if aliased\n",
    "            try:\n",
    "                single_df = df.filter(col(\"description\") == desc)\n",
    "                single_df = single_df.with_column(\"ner_json\", expr(qq))\n",
    "                single_df.select(\"ner_json\").show()\n",
    "                rcount += 1\n",
    "            except Exception as e:\n",
    "                ecount += 1\n",
    "                print(f\"Errore {ecount} su: {desc}\")\n",
    "        print(f\"Numero totale di errori: {ecount}\")\n",
    "        '''\n",
    "\n",
    "        df_with_ner = session.sql(query).collect()\n",
    "        df_with_ner = df_with_ner.filter(df_with_ner[\"ner_json\"].is_not_null())\n",
    "        df_with_ner = df_with_ner.filter(\n",
    "            (col(\"ner_json\")[\"response\"].is_not_null()) &\n",
    "            (trim(col(\"ner_json\")[\"response\"]) != \"\")\n",
    "            )\n",
    "\n",
    "        df_with_ner = df_with_ner.with_columns(\n",
    "            [\"age\", \"date_of_birth\", \"location\", \"zip_code\", \"last_job\", \"second_last_job\", \"third_last_job\", \"skills\"],\n",
    "            [\n",
    "                col(\"ner_json\")[\"response\"][\"età\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"response\"][\"data_di_nascita\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"response\"][\"località\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"response\"][\"cap\"].cast(\"STRING\"), \n",
    "                col(\"ner_json\")[\"response\"][\"ultimo_lavoro\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"response\"][\"penultimo_lavoro\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"response\"][\"terzultimo_lavoro\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"response\"][\"competenze\"].cast(\"STRING\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        df_with_ner = df_with_ner.drop(\"ner_json\")\n",
    "\n",
    "        #print(df_with_ner.show(100))\n",
    "        logger.info(f\"NER on {input_table} table successful\")\n",
    "\n",
    "\n",
    "        return df_with_ner\n",
    "\n",
    "\n",
    "\n",
    "    def apply_ner_cortexai(self, session):\n",
    "        \"\"\"\n",
    "        Reads input table\n",
    "\n",
    "        Performs Named Entity Recognition\n",
    "        Function returns Snowflake dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        from snowflake.snowpark.functions import parse_json\n",
    "\n",
    "\n",
    "        database = self.config.database\n",
    "        schema = self.config.schema\n",
    "        input_table_cleaned = self.config.input_table_cleaned\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT\n",
    "                *,\n",
    "                SNOWFLAKE.CORTEX.COMPLETE(\n",
    "                    'claude-4-sonnet',\n",
    "                    CONCAT(\n",
    "                        'Estrai dal seguente testo i campi: age (numero), date_of_birth (YYYY-MM-DD), location (stringa), zip_code (numero), last_job, second_last_job, third_last_job, skills (stringa). ',\n",
    "                        'Rispondi in formato JSON, senza testo extra, attieniti a questo esempio: {{\"age\": 30, \"date_of_birth\": \"1993-05-12\", \"location\": \"Milano\", \"zip_code\": 20100, \"last_job\": \"Data Engineer\", \"second_last_job\": \"Developer\", \"third_last_job\": \"Intern\", \"skills\": \"Python, SQL\"}}. ',\n",
    "                        'Testo: ', description\n",
    "                    )\n",
    "                ) AS ner_json\n",
    "            FROM {database}.{schema}.{input_table_cleaned}\n",
    "\n",
    "            \"\"\"\n",
    "         \n",
    "        import json\n",
    "\n",
    "        response_format = {\n",
    "            \"età\": \"Qual è l età del candidato? (numero)\",\n",
    "            \"data_di_nascita\": \"Qual e la data di nascita del candidato? Restituisci solo uno in formato stringa ANNO-MESE-GIORNO\",\n",
    "            \"località\": \"Qual è la località del candidato? Restituisci solo uno in formato stringa\",\n",
    "            \"cap\": \"Qual è il codice postale del candidato? Restituisci solo uno in formato stringa\",\n",
    "            \"ultimo_lavoro\": \"Qual è l ultimo lavoro svolto dal candidato? Restituisci solo uno in formato stringa, il tipo di lavoro, non l azienda o il datore di lavoro\",\n",
    "            \"penultimo_lavoro\": \"Qual è il penultimo lavoro svolto dal candidato? Restituisci solo uno in formato stringa, il tipo di lavoro, non l azienda o il datore di lavoro\",\n",
    "            \"terzultimo_lavoro\": \"Qual è il terzultimo lavoro svolto dal candidato? Restituisci solo uno in formato stringa, il tipo di lavoro, non l azienda o il datore di lavoro\",\n",
    "            \"competenze\": \"Qual è l elenco delle competenze del candidato? (stringa di competenze separate da virgola)\"\n",
    "        }\n",
    "\n",
    "        response_json = json.dumps(response_format)\n",
    "\n",
    "        qq = \"\"\" \n",
    "                SNOWFLAKE.CORTEX.COMPLETE(\n",
    "                    'claude-4-sonnet',\n",
    "                    CONCAT(\n",
    "                        'Estrai dal seguente testo i campi: age (numero), date_of_birth (YYYY-MM-DD), location (stringa), zip_code (numero), last_job, second_last_job, third_last_job, skills (stringa). ',\n",
    "                        'Rispondi in formato JSON, senza testo extra, attieniti a questo esempio: {{\"age\": 30, \"date_of_birth\": \"1993-05-12\", \"location\": \"Milano\", \"zip_code\": 20100, \"last_job\": \"Data Engineer\", \"second_last_job\": \"Developer\", \"third_last_job\": \"Intern\", \"skills\": \"Python, SQL\"}}. ',\n",
    "                        'Testo: ', description\n",
    "                    )\n",
    "                )\n",
    "                \"\"\"\n",
    "\n",
    "        '''\n",
    "        df = session.sql(f\"SELECT * FROM {database}.{schema}.{input_table_cleaned}\")\n",
    "        descriptions = df.select(col(\"description\").alias(\"description\")).collect()\n",
    "        rcount = ecount = 0\n",
    "        for row in descriptions:\n",
    "            desc = row[0]  # or row[\"description\"] if aliased\n",
    "            try:\n",
    "                single_df = df.filter(col(\"description\") == desc)\n",
    "                single_df = single_df.with_column(\"ner_json\", expr(qq))\n",
    "                single_df.select(\"ner_json\").show()\n",
    "                rcount += 1\n",
    "            except Exception as e:\n",
    "                ecount += 1\n",
    "                print(f\"Errore {ecount} su: {desc}\")\n",
    "        print(f\"Numero totale di errori: {ecount}\")\n",
    "        '''\n",
    "\n",
    "        df = session.sql(query)\n",
    "\n",
    "\n",
    "        def clean_parsing_text_udf(x: str) -> str:\n",
    "            if x is None:\n",
    "                return ''\n",
    "            x = x.lower().lstrip()\n",
    "\n",
    "            if x.startswith(\"```json\"):\n",
    "                x = x[8:].lstrip()\n",
    "\n",
    "            x = x.replace('\\n', ' ').replace('\\t', ' ').replace('\\\\', '').strip()\n",
    "            x = ' '.join(x.split())\n",
    "\n",
    "            if x.endswith(\"```\"):\n",
    "                x = x[:-3].rstrip()\n",
    "            if x.endswith(\"'\") or x.endswith('\"'):\n",
    "                x = x[:-1].rstrip()\n",
    "\n",
    "            return x\n",
    "\n",
    "        clean_udf = udf(\n",
    "            clean_parsing_text_udf,\n",
    "            return_type=StringType(),\n",
    "            input_types=[StringType()]\n",
    "        )\n",
    "\n",
    "        df = df.with_column(\"ner_json\", clean_udf(df[\"ner_json\"]))\n",
    "\n",
    "        df = df.with_column(\"ner_json\", parse_json(col(\"ner_json\")))\n",
    "\n",
    "        print(df.select(col(\"ner_json\")[\"age\"]).show())\n",
    "\n",
    "        df = df.filter(df[\"ner_json\"].is_not_null())\n",
    "\n",
    "        '''\n",
    "        df = df.filter(\n",
    "            (col(\"ner_json\")[\"response\"].is_not_null()) &\n",
    "            (trim(col(\"ner_json\")[\"response\"]) != \"\")\n",
    "            )\n",
    "        '''\n",
    "        df = df.with_columns(\n",
    "            [\"age\", \"date_of_birth\", \"location\", \"zip_code\", \"last_job\", \"second_last_job\", \"third_last_job\", \"skills\"],\n",
    "            [\n",
    "                col(\"ner_json\")[\"age\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"data_of_birth\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"location\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"zip_code\"].cast(\"STRING\"), \n",
    "                col(\"ner_json\")[\"last_job\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"second_last_job\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"third_last_job\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"skills\"].cast(\"STRING\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        df = df.drop(\"ner_json\")\n",
    "\n",
    "        print(df.show(100))\n",
    "        logger.info(f\"NER on {input_table_cleaned} table successful\")\n",
    "\n",
    "\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def apply_ner_rowwise(self, session):\n",
    "\n",
    "        from snowflake.snowpark import Row\n",
    "        from snowflake.snowpark.functions import lit\n",
    "        from functools import reduce\n",
    "\n",
    "        database = self.config.database\n",
    "        schema = self.config.schema\n",
    "        input_table_cleaned = self.config.input_table_cleaned\n",
    "\n",
    "        response_format = {\n",
    "            \"età\": \"Qual è l età del candidato? (numero)\",\n",
    "            \"data_di_nascita\": \"Qual e la data di nascita del candidato? Restituisci solo uno in formato stringa ANNO-MESE-GIORNO\",\n",
    "            \"località\": \"Qual è la località del candidato? Restituisci solo uno in formato stringa\",\n",
    "            \"cap\": \"Qual è il codice postale del candidato? Restituisci solo uno in formato stringa\",\n",
    "            \"ultimo_lavoro\": \"Qual è l ultimo lavoro svolto dal candidato? Restituisci solo uno in formato stringa, il tipo di lavoro, non l azienda o il datore di lavoro\",\n",
    "            \"penultimo_lavoro\": \"Qual è il penultimo lavoro svolto dal candidato? Restituisci solo uno in formato stringa, il tipo di lavoro, non l azienda o il datore di lavoro\",\n",
    "            \"terzultimo_lavoro\": \"Qual è il terzultimo lavoro svolto dal candidato? Restituisci solo uno in formato stringa, il tipo di lavoro, non l azienda o il datore di lavoro\",\n",
    "            \"competenze\": \"Qual è l elenco delle competenze del candidato? (stringa di competenze separate da virgola)\"\n",
    "        }\n",
    "\n",
    "        import json\n",
    "        response_json = json.dumps(response_format)\n",
    "\n",
    "        rows = session.table(f\"{database}.{schema}.{input_table_cleaned}\").collect()\n",
    "        results = []\n",
    "        for row in rows:\n",
    "            try:\n",
    "                candidate_id = row[\"CANDIDATEID\"]\n",
    "                description = row[\"description\"]\n",
    "\n",
    "                df_single = session.create_dataframe(\n",
    "                    [[candidate_id, description]],\n",
    "                    schema=[\"CANDIDATEID\", \"description\"]\n",
    "                )\n",
    "\n",
    "                df_single = df_single.with_column(\n",
    "                    \"ner_json\",\n",
    "                    expr(f\"\"\"\n",
    "                        AI_EXTRACT(\n",
    "                            text => description,\n",
    "                            responseFormat => PARSE_JSON('{response_json}')\n",
    "                        )\n",
    "                    \"\"\")\n",
    "                )\n",
    "\n",
    "                results.append(df_single)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Errore su CANDIDATEID={candidate_id}: {e}\")\n",
    "\n",
    "        if not results:\n",
    "            raise ValueError(\"Nessuna riga è stata processata correttamente da AI_EXTRACT.\")\n",
    "\n",
    "        final_df = reduce(lambda a, b: a.union(b), results)\n",
    "\n",
    "        final_df = final_df.with_columns(\n",
    "            [\"age\", \"date_of_birth\", \"location\", \"zip_code\", \"last_job\", \"second_last_job\", \"third_last_job\", \"skills\"],\n",
    "            [\n",
    "                col(\"ner_json\")[\"response\"][\"età\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"response\"][\"data_di_nascita\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"response\"][\"località\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"response\"][\"cap\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"response\"][\"ultimo_lavoro\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"response\"][\"penultimo_lavoro\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"response\"][\"terzultimo_lavoro\"].cast(\"STRING\"),\n",
    "                col(\"ner_json\")[\"response\"][\"competenze\"].cast(\"STRING\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        logger.info(\"NER row-wise completato con successo\")\n",
    "        return final_df\n",
    "\n",
    "\n",
    "    def write_table(self, session, df):\n",
    "        \"\"\"\n",
    "        Writes input table\n",
    "        Function returns nothing\n",
    "        \"\"\"\n",
    "        output_table = self.config.output_table\n",
    "\n",
    "        df.write.save_as_table(output_table, mode=\"overwrite\")\n",
    "        logger.info(f\"Table {output_table} successfully written\")\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4352f8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-07 11:51:12,889: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-10-07 11:51:12,893: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-10-07 11:51:12,897: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-10-07 11:51:12,901: INFO: common: created directory at: artifacts]\n",
      "[2025-10-07 11:51:12,903: INFO: common: created directory at: artifacts/data_transformation]\n",
      "-------------------------\n",
      "|\"\"\"NER_JSON\"\"['AGE']\"  |\n",
      "-------------------------\n",
      "|36                     |\n",
      "|null                   |\n",
      "|22                     |\n",
      "|37                     |\n",
      "|null                   |\n",
      "|18                     |\n",
      "|54                     |\n",
      "|24                     |\n",
      "|22                     |\n",
      "|23                     |\n",
      "-------------------------\n",
      "\n",
      "None\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"DATE_ADDED\"  |\"CANDIDATEID\"  |\"DESCRIPTION\"  |\"AGE\"  |\"DATE_OF_BIRTH\"  |\"LOCATION\"  |\"ZIP_CODE\"  |\"LAST_JOB\"  |\"SECOND_LAST_JOB\"  |\"THIRD_LAST_JOB\"  |\"SKILLS\"  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|              |               |               |       |                 |            |            |            |                   |                  |          |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "None\n",
      "[2025-10-07 11:51:45,784: INFO: 431447175: NER on MPG_IT_AUTOMATCH_CANDIDATE_CLEANED table successful]\n",
      "[2025-10-07 11:51:59,235: INFO: 431447175: Table MPG_IT_AUTOMATCH_CANDIDATE_FEATURES successfully written]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    #df = data_transformation.clean_description(session)\n",
    "    df = data_transformation.apply_ner_cortexai(session)\n",
    "    data_transformation.write_table(session, df)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b1870ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE_a</th>\n",
       "      <th>AGE_b</th>\n",
       "      <th>CANDIDATEID</th>\n",
       "      <th>DATE_ADDED</th>\n",
       "      <th>DATE_OF_BIRTH_a</th>\n",
       "      <th>DATE_OF_BIRTH_b</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>LAST_JOB_a</th>\n",
       "      <th>LAST_JOB_b</th>\n",
       "      <th>LOCATION_a</th>\n",
       "      <th>LOCATION_b</th>\n",
       "      <th>SECOND_LAST_JOB_a</th>\n",
       "      <th>SECOND_LAST_JOB_b</th>\n",
       "      <th>SKILLS_a</th>\n",
       "      <th>SKILLS_b</th>\n",
       "      <th>THIRD_LAST_JOB_a</th>\n",
       "      <th>THIRD_LAST_JOB_b</th>\n",
       "      <th>ZIP_CODE_a</th>\n",
       "      <th>ZIP_CODE_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [AGE_a, AGE_b, CANDIDATEID, DATE_ADDED, DATE_OF_BIRTH_a, DATE_OF_BIRTH_b, DESCRIPTION, LAST_JOB_a, LAST_JOB_b, LOCATION_a, LOCATION_b, SECOND_LAST_JOB_a, SECOND_LAST_JOB_b, SKILLS_a, SKILLS_b, THIRD_LAST_JOB_a, THIRD_LAST_JOB_b, ZIP_CODE_a, ZIP_CODE_b]\n",
       "Index: []"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a = session.sql(\"SELECT * FROM MPG_IT_AUTOMATCH_CANDIDATE_FEATURES\").to_pandas()\n",
    "df_b = session.sql(\"SELECT * FROM AUTOMATCH_FINAL_TABLE\").to_pandas()\n",
    "\n",
    "df_a[\"CANDIDATEID\"] = df_a[\"CANDIDATEID\"].astype(str)\n",
    "df_b[\"CANDIDATEID\"] = df_b[\"CANDIDATEID\"].astype(str)\n",
    "\n",
    "df_merged = pd.merge(df_a, df_b, on=\"CANDIDATEID\", suffixes=(\"_a\", \"_b\"))\n",
    "df_merged = df_merged[sorted(df_merged.columns)]\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4fbeba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"response\": {\n",
      "    \"address\": \"Milano\",\n",
      "    \"name\": \"Antonio\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = session.sql(f\"\"\"SELECT AI_EXTRACT(\n",
    "  text => 'Antonio Cavalli Software Engineer Location: Milano',\n",
    "  responseFormat => PARSE_JSON('{{\"name\": \"What is the first name of the employee?\", \"address\": \"What is the address of the employee?\"}}')\n",
    " \n",
    ");\"\"\").collect()\n",
    "print(result[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6921d9e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ai_extract' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mai_extract\u001b[49m(\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJohn Smith lives in San Francisco and works for Snowflake\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat is the first name of the employee?\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat is the address of the employee?\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m      5\u001b[0m     )\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextracted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ai_extract' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "df = session.range(1).select(\n",
    "    ai_extract(\n",
    "        'John Smith lives in San Francisco and works for Snowflake',\n",
    "        {'name': 'What is the first name of the employee?', 'city': 'What is the address of the employee?'}\n",
    "    ).alias(\"extracted\")\n",
    ")\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
