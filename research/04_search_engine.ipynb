{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875de2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\fiscarelli\\\\Desktop\\\\Progetti\\\\Manpower IT\\\\Auto-Match\\\\Candidates-to-Jobs-Auto-Match-Cortex-AI'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"snowflake\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"snowflake.snowpark\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "\n",
    "# Add the absolute path to src/ so Python can find automatch\n",
    "src_path = os.path.abspath(\"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c1705f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\fiscarelli\\\\Desktop\\\\Progetti\\\\Manpower IT\\\\Auto-Match\\\\Candidates-to-Jobs-Auto-Match-Cortex-AI'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9155a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n",
      "Going to open: https://login.microsoftonline.com/e2ba81b8-03fe-407c-96a1-f4bc0f512e7d/saml2?SAMLRequest=nZJPb%2BIwEMW%2FSuQ9J3FSoGABFW0WNVK3i0q62uXmJJPUwrFT%2FyG0n36dAFL30B72Ztlv5vc8b%2BY3x4Z7B1CaSbFAUYCRB6KQJRP1Aj1na3%2BKPG2oKCmXAhboDTS6Wc41bXhLVta8iCd4taCN5xoJTfqHBbJKEEk100TQBjQxBdmufjyQOMCEag3KOBw6l5SaOdaLMS0Jw67rgu4qkKoOY4xxiGehU%2FWSb%2BgDov2a0SppZCH5peTo%2FvQJIgrxqEc4hSNszoW3TJxG8BUlP4k0uc%2Byjb%2F5uc2Qt7r87k4KbRtQW1AHVsDz08PJgHYObrezEY6nQefm5oNVsoWAvlsFgRayqzjdQyGb1hrXPXCnsIIy5LJmbmZpskDtnpW4zHc2eU3eowme2cN6J22e7DGv%2F9Qprb%2Bb4%2F3vKMkea5rWBfJ%2BXRKO%2B4RTrS2kos%2FVuCscj%2F0I%2BzHOopiMp2SMg8k03iEvcf6YoGaovJgffAQNK5TUsjJScCZgcAlxTqdRPvXxVQX%2BCF8X%2FmxCI78a5QWuxlEM12XYpxej0waRwYha%2Fu9c5uHHLuelfHQ5pclGcla8eWupGmo%2BjzEKouGGlX41SAk0lPFVWSrQ2sXJuezuFFDjdt8oCyhcnqj%2Fbv%2FyLw%3D%3D&RelayState=ver%3A1-hint%3A126482533224670-ETMsDgAAAZoBs3m6ABRBRVMvQ0JDL1BLQ1M1UGFkZGluZwEAABAAEPSWz5uTgfOMe7qw83Wxcb8AAACguRRZhiF95mrcJ9TDAIY%2BBLflqXdRuTMYN8bIhH%2BNgV2u4hAjJKI3wWVzGviqG6Z1f6NgN2m%2B3ankfWd%2BS0q%2FduRwaKPau4bSl7h0ZXiyaragOcYclxCMf3p8jFuB5EhoAbPyuvIc0Kx6ri3QUnZmz8Y7EkT3mKEyDN%2BW%2FvUrjLeEhgIwaghTR5pBMiNQRFbIWE%2BhxQFZzYZ0RsITvd84tAAU1nT18cm8owqt76JK9%2FBSxFruj2o%3D to authenticate...\n"
     ]
    }
   ],
   "source": [
    "from autoMatch.utils.snowflake_utils import get_snowpark_session\n",
    "session = get_snowpark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc40000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SearchEngineConfig:\n",
    "    root_dir: str\n",
    "    database: str\n",
    "    schema: str\n",
    "    input_table: str\n",
    "    search_columns : dict\n",
    "    attributes_columns: dict\n",
    "    columns: dict\n",
    "    search_service: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdecf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoMatch.constants import *\n",
    "from autoMatch.utils.common import read_yaml, create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_search_engine_config(self) -> SearchEngineConfig:\n",
    "        config = self.config.search_engine\n",
    "        schema = self.schema.search_engine\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        search_engine_config = SearchEngineConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            database=config.database,\n",
    "            schema=config.schema,\n",
    "            input_table=config.input_table,\n",
    "            search_service=config.search_service,\n",
    "            search_columns=schema.search_columns,\n",
    "            attributes_columns = schema.attributes_columns,\n",
    "            columns = schema.columns\n",
    "        )\n",
    "\n",
    "        return search_engine_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af745682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoMatch import logger\n",
    "from dotenv import load_dotenv\n",
    "from snowflake.core import Root\n",
    "\n",
    "        \n",
    "class SearchEngine:\n",
    "    def __init__(self, config: SearchEngineConfig):\n",
    "        self.config = config\n",
    "\n",
    "   \n",
    "    def create_semantic_search_engine(self, session):\n",
    "        \"\"\"\n",
    "        Creates Cortex Search Service on input table\n",
    "\n",
    "        Function returns nothing\n",
    "        \"\"\"\n",
    "\n",
    "        database = self.config.database\n",
    "        schema = self.config.schema\n",
    "        input_table = self.config.input_table\n",
    "        search_service = self.config.search_service\n",
    "        search_columns = self.config.search_columns\n",
    "        attributes_columns = self.config.attributes_columns\n",
    "        columns = self.config.columns\n",
    "\n",
    "        load_dotenv()\n",
    "        warehouse = os.getenv(\"SNOWFLAKE_WAREHOUSE\")\n",
    "\n",
    "        description_expr = \" || '\\\\n' || \".join([\n",
    "            f\"'{' '.join(col.strip().split('_')).capitalize()}: ' || {col.strip()}\"\n",
    "            for col in search_columns\n",
    "        ]) \n",
    "               \n",
    "        query = f\"\"\"\n",
    "            CREATE OR REPLACE CORTEX SEARCH SERVICE {search_service}\n",
    "            ON description\n",
    "            ATTRIBUTES {\", \".join(attributes_columns)}\n",
    "            WAREHOUSE = {warehouse}\n",
    "            TARGET_LAG = '1 hour'\n",
    "            AS (\n",
    "                SELECT {\", \".join(columns)},\n",
    "                ({description_expr}) as description\n",
    "                FROM {database}.{schema}.{input_table}\n",
    "                );\n",
    "            \"\"\"\n",
    "        \n",
    "        logger.info(f\"Cortex search index {search_service} successfully defined on table {input_table}\")\n",
    "\n",
    "        session.sql(query).collect()\n",
    "  \n",
    "        \n",
    "    def get_column_specification(self, session):\n",
    "        \"\"\"\n",
    "        Get Search Service columns:\n",
    "            - search columns\n",
    "            - attribute columns\n",
    "            - all columns involved\n",
    "        Function returns nothin\n",
    "        \"\"\"\n",
    "\n",
    "        database = self.config.database\n",
    "        schema = self.config.schema\n",
    "        search_service = self.config.search_service\n",
    "\n",
    "        search_service_result = session.sql(f\"DESC CORTEX SEARCH SERVICE {database}.{schema}.{search_service}\").collect()[0]\n",
    "        attribute_columns = search_service_result.attribute_columns.split(\",\")\n",
    "        search_columns = search_service_result.search_column\n",
    "        columns = search_service_result.columns.split(\",\")\n",
    "\n",
    "        logger.info(f\"Column specifications: \\nSearch columns: {search_columns} \\nAttribute columns: {attribute_columns} \\nAll columns: {columns}\")\n",
    "\n",
    "        return search_service, attribute_columns, search_columns, columns\n",
    "    \n",
    "    def query_cortex_search_service(self, session, query, filter={}, limit=5):\n",
    "        \"\"\"\n",
    "        Queries the cortex search service in the session state and returns a list of results\n",
    "\n",
    "        Returns query results\n",
    "        \"\"\"\n",
    "\n",
    "        database = self.config.database\n",
    "        schema = self.config.schema\n",
    "        search_service = self.config.search_service\n",
    "        columns = self.config.columns\n",
    "\n",
    "        #_, _, _, columns = self.get_column_specification(session)\n",
    "\n",
    "        cortex_search_service = (\n",
    "            Root(session)\n",
    "            .databases[database]\n",
    "            .schemas[schema]\n",
    "            .cortex_search_services[search_service]\n",
    "        )\n",
    "        context_documents = cortex_search_service.search(\n",
    "            query,\n",
    "            columns=columns,\n",
    "            filter=filter,\n",
    "            limit=limit)\n",
    "        \n",
    "        return context_documents.results\n",
    "    \n",
    "    def create_filter(self, max_age): #, skills):\n",
    "        \"\"\"\n",
    "        Create a filter object to include only candidates with:\n",
    "        - age <= max_age\n",
    "        - AND all specified skills present in the 'skills' string column\n",
    "        \"\"\"\n",
    "        filter_clauses = []\n",
    "\n",
    "        # Age clause (directly append the valid @lte clause)\n",
    "        age_clause = { \"@lte\": { \"age\": max_age } }\n",
    "        filter_clauses.append(age_clause)\n",
    "\n",
    "        # Skills clause: all skills must be present in the string\n",
    "        #skill_and_clauses = [{ \"@contains\": { \"skills\": skill } } for skill in skills]\n",
    "        #filter_clauses.extend(skill_and_clauses)\n",
    "\n",
    "        return { \"@and\": filter_clauses }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d69f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-20 14:58:56,051: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-10-20 14:58:56,055: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-10-20 14:58:56,061: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-10-20 14:58:56,061: INFO: common: created directory at: artifacts]\n",
      "[2025-10-20 14:58:56,061: INFO: common: created directory at: artifacts/search_engine]\n",
      "[2025-10-20 14:58:56,067: INFO: 3324900363: Cortex search index CANDIDATE_SEARCH_SERVICE successfully defined on table MPG_IT_AUTOMATCH_CANDIDATE_FEATURES]\n",
      "[2025-10-20 15:00:12,821: INFO: 3324900363: Column specifications: \n",
      "Search columns: DESCRIPTION \n",
      "Attribute columns: ['AGE'] \n",
      "All columns: ['CANDIDATEID', 'AGE', 'LOCATION', 'LAST_JOB', 'SECOND_LAST_JOB', 'THIRD_LAST_JOB', 'SKILLS', 'DESCRIPTION']]\n",
      "{'skills': 'oracle, informatica power center, kafka, powerbi, qliksense, apex, hadoop, hdfs, hive, impala, hbase, solr, splunk, sql server, java, sql, xml, python', '@scores': {'text_match': 0.03963147, 'cosine_similarity': 0.49051362}, 'third_last_job': 'senior data warehouse & business intelligence developer', 'location': 'bologna', 'second_last_job': 'senior data engineer & data architect', 'candidateid': '5527925', 'last_job': 'data platform & analytics engineer', 'age': '37'}\n",
      "{'skills': 'python, r, sql, postgresql, machine learning, git, linux, java, apache spark, mysql, stata, numpy, pandas, scikit-learn, matplotlib', '@scores': {'text_match': 0.122772224, 'cosine_similarity': 0.5185976}, 'third_last_job': 'ricercatore statistico', 'location': 'roma', 'second_last_job': 'analista funzionale', 'candidateid': '5562519', 'last_job': 'data scientist', 'age': '33'}\n",
      "{'skills': 'sql, python, pyspark, git, dbt, databricks, snowflake, microsoft powerbi, google looker, dax, microsoft excel', '@scores': {'text_match': 0.009022786, 'cosine_similarity': 0.4962843}, 'third_last_job': 'responsabile web marketing', 'location': 'bologna', 'second_last_job': 'analista dati e tecnico seo e online advertising', 'last_job': 'data management specialist', 'candidateid': '5540308', 'age': '33'}\n",
      "{'skills': 'spring boot, api restful, soap, microservizi, xml/json, mirth connect, c#, php, mysql, yii2, symfony, oracle, postgresql, genero 4gl, java, c++, tcp/ip, http, https, opengl, webgl, crittografia, sicurezza digitale', '@scores': {'text_match': 0.01005202, 'cosine_similarity': 0.5006443}, 'third_last_job': 'analista sviluppatore', 'location': 'palermo', 'second_last_job': 'co.co.co. collaboratore professionale ingegnere', 'candidateid': '5563231', 'last_job': 'consulente esterno con partita iva ingegnere informatico', 'age': '35'}\n",
      "{'skills': 'python, r, sql, data visualization, web analytics, business intelligence, pacchetto office, social network, comunicazione, gestione progetti', '@scores': {'text_match': 0.013827296, 'cosine_similarity': 0.48602533}, 'third_last_job': 'informatore turistico', 'location': 'paola', 'second_last_job': 'articolista', 'last_job': 'data analyst', 'candidateid': '5527020', 'age': '36'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTODO\\n\\n1. search service as is, filter on age. then crate app\\n2. add hard skills in the filter (must convert skills column from a string of comma-separated skills to a vector of skills)\\n3. add distance_km column in data_transformation and include it when creating the search service, then compute it based on location inserted in the app. and use it as filter\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    search_engine_config = config.get_search_engine_config()\n",
    "    search_engine = SearchEngine(config=search_engine_config)\n",
    "    #search_engine.create_semantic_search_engine(session)\n",
    "    #search_engine.get_column_specification(session)\n",
    "    results = search_engine.query_cortex_search_service(session, \n",
    "                                                        query='Data Scientist con esperienza in Kafka', \n",
    "                                                        filter=search_engine.create_filter(40), \n",
    "                                                        limit=5)\n",
    "    for result in results:\n",
    "        print(result)\n",
    "        print(\"\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise e\n",
    "\n",
    "'''\n",
    "TODO\n",
    "\n",
    "1. search service as is, filter on age. then crate app\n",
    "2. add distance_km column in data_transformation and include it when creating the search service, then compute it based on location inserted in the app. and use it as filter\n",
    "3. add hard skills in the filter (must convert skills column from a string of comma-separated skills to a vector of skills)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aaafd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DESCRIPTION=\"\\n\\n\\n\\n\\n\\nurbano arotce \\xa0\\ninformazioni personali \\xa0\\nmi sono trasferito recentemente in italia con la mia compagna, in cerca di nuove \\xa0\\nopportunità professionali e di crescita personale. sono una persona attenta, \\xa0\\nproattiva e fessibile, con una forte predisposizione al lavoro di squadra e al \\xa0\\ncontatto con il pubblico. mi adatto facilmente a nuovi contesti e sono determinato a \\xa0\\ncostruire un percorso solido e signifcativo nel ámbito lavorativo. \\xa0\\nesperienza professionale \\xa0\\nvia pianezza 82 torino \\xa0\\n3447663563 \\xa0\\ncameriere \\xa0\\nla gringa pizza e pasta\\xa0pilar, argentina \\xa0\\ngennaio 2008 - aprile 2009 \\xa0\\npcoutletderqui@gmail.com \\xa0\\ndata di nascita : 21-08-1989 \\xa0\\nnazionalità : italiana - argentina \\xa0\\nsesso : maschile \\xa0\\nho lavorato come cameriere occupandomi del servizio ai tavoli, della pulizia e della preparazione delle \\xa0\\npostazioni. \\xa0\\naddetto alle vendite \\xa0\\nwicomp computacion\\xa0pilar, argentina \\xa0\\nagosto 2009 - marzo 2012 \\xa0\\nho lavorato come cassiere in un negozio di informatica, occupandomi dell’assistenza e del pagamento \\xa0\\ndei clienti, della gestione dei reclami, nonché della riparazione e vendita di computer. \\xa0\\npatente : a1 \\xa0\\nassistente di ufcio generale \\xa0\\nacerbrag s.a.\\xa0pilar, argentina \\xa0\\naprile 2012 - marzo 2014 \\xa0\\nlingue \\xa0\\nriconciliazioni bancarie e era responsabile dell'archivio. inserimento dati \\xa0\\nspagnolo \\xa0\\nmadrelìngua \\xa0\\ncassiere \\xa0\\npapelera comaso s.r.l.\\xa0pilar, argentina \\xa0\\ninglese \\xa0\\nagosto 2014 - marzo 2020 \\xa0\\nc2 \\xa0\\ncompravendita di articoli monouso. acquisto, preparazione degli ordini e consegna. acquisizione di \\xa0\\nclienti. \\xa0\\nitaliano \\xa0\\na2 \\xa0\\nmagazziniere \\xa0\\nloginorte s.a.\\xa0pilar, argentina \\xa0\\ncompetenze \\xa0\\nexcel \\xa0\\naprile 2020 - aprile 2023 \\xa0\\nresponsabile della migrazione dei fogli excel a un sistema wms basato su sap. gestione dello stock, \\xa0\\nredazione di protocolli per merci pericolose. assistenza clienti (ecolab). \\xa0\\ngestione della \\xa0\\ncassa \\xa0\\ncassiere \\xa0\\ncarrefour market\\xa0pilar, argentina \\xa0\\nmaggio 2023 - maggio 2025 \\xa0\\nassistenza clienti \\xa0\\nho lavorato alla cassa presso carrefour, occupandomi dell’incasso dei pagamenti, della scansione dei \\xa0\\nprodotti e dell’assistenza ai clienti durante il processo di acquisto. \\xa0\\ngestione di \\xa0\\nreclami \\xa0\\nistruzione e formazione \\xa0\\ncomunicazione \\xa0\\nefcace \\xa0\\ndiploma di maturità \\xa0\\nescuela media 7°\\xa0pilar, argentina \\xa0\\ngennaio 1991 - novembre 2007 \\xa0\\ngestione del \\xa0\\nmagazzino \\xa0\\ncertifcati \\xa0\\ninventario e \\xa0\\nregistrazione \\xa0\\ndiploma di maturità \\xa0\\ncertifcato di inglese – ef set : https://cert.efset.org/mubzqa \\xa0\\ncorso di programmazione in python e javascript – argentina programa \\xa0\\ncompetenze \\xa0\\ndigitali generali \\xa0\\nautorizzo il trattamento dei miei dati personali ai sensi del regolamento ue \\xa0\\n2016/679 (gdpr) per fnalità di selezione del personale. \\xa0\\nurbano arotce \\xa0\\npagina 1/2 \\xa0\\n\\n\\n\")]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"\"\"select description \n",
    "            from IT_DISCOVERY.CONSUMER_INT_MODEL.MPG_IT_AUTOMATCH_CANDIDATE_CLEANED\n",
    "            where candidateid = '5544264'\n",
    "            \"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250ec554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(RESULTS='[\\n  {\\n    \"@scores\": {\\n      \"cosine_similarity\": 0.58208793,\\n      \"text_match\": 0.076404855\\n    },\\n    \"last_job\": \"full stack developer & data scientist\",\\n    \"second_last_job\": \"data scientist\",\\n    \"skills\": \"python, flask, fastapi, postgresql, mongodb, api rest, javascript, react, html5, css3, jquery, figma, adobe xd, aws, azure databricks, git, docker, tableau, pandas, scipy, matplotlib, llm, openai, rag, scikit-learn, machine learning\"\\n  },\\n  {\\n    \"@scores\": {\\n      \"cosine_similarity\": 0.5920123,\\n      \"text_match\": 0.023447132\\n    },\\n    \"last_job\": \"data analyst\",\\n    \"second_last_job\": \"executive engineer\",\\n    \"skills\": \"python, javascript, java, c/c++, sql, c#, html, css, pandas, numpy, power bi, tableau, matplotlib, sql server, mongodb, sqlite, pytorch, hugging face transformers, spacy, bert, scikit-learn, q-learning, genetic algorithms, svm, gmm, logistic regression, weights & biases, runpod, databricks, firebase, react, node.js, express, django, langchain, hadoop, spark, git, jupyter, vs code, docker, agile/scrum\"\\n  },\\n  {\\n    \"@scores\": {\\n      \"cosine_similarity\": 0.57923293,\\n      \"text_match\": 0.070882864\\n    },\\n    \"last_job\": \"ai & automation intern\",\\n    \"second_last_job\": \"data scientist / ai engineer\",\\n    \"skills\": \"python, c#, c, c++, java, javascript, php, dart, tensorflow, pytorch, hugging face, scikit-learn, llms, langchain, pandas, numpy, pyspark, asp.net core, fastapi, flask, sql, mongodb, mysql, git, jenkins, selenium, aws, flutter\"\\n  },\\n  {\\n    \"@scores\": {\\n      \"cosine_similarity\": 0.5476924,\\n      \"text_match\": 0.070882864\\n    },\\n    \"last_job\": \"smart passenger information system\",\\n    \"second_last_job\": \"data scientist\",\\n    \"skills\": \"python, java, sql, scikit-learn, tensorflow, keras, pytorch, oracle sql, mysql, mongodb, pyspark, microsoft power bi, tableau, matplotlib, seaborn, git, microsoft excel, ibm spss modeler, minitab\"\\n  },\\n  {\\n    \"@scores\": {\\n      \"cosine_similarity\": 0.5535383,\\n      \"text_match\": 0.16190997\\n    },\\n    \"last_job\": \"data scientist\",\\n    \"second_last_job\": \"analista funzionale\",\\n    \"skills\": \"python, r, sql, postgresql, machine learning, git, linux, java, apache spark, mysql, stata, numpy, pandas, scikit-learn, matplotlib\"\\n  }\\n]')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "session.sql(\"\"\"\n",
    "SELECT PARSE_JSON(\n",
    "  SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n",
    "      'CANDIDATE_SEARCH_SERVICE',\n",
    "'{\n",
    "        \"query\": \"Data Scientist with experience in Python and Java \",\n",
    "        \"columns\":[\n",
    "            \"last_job\",\n",
    "            \"second_last_job\",\n",
    "            \"skills\"\n",
    "        ],\n",
    "        \"limit\":5\n",
    "      }'\n",
    "  )\n",
    ")['results'] as results;\n",
    "\"\"\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
