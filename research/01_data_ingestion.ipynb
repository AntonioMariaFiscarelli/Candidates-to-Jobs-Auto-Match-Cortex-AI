{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875de2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\fiscarelli\\\\Desktop\\\\Progetti\\\\Manpower IT\\\\Auto-Match\\\\Candidates-to-Jobs-Auto-Match-Cortex-AI'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "\n",
    "# Add the absolute path to src/ so Python can find automatch\n",
    "src_path = os.path.abspath(\"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c1705f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\fiscarelli\\\\Desktop\\\\Progetti\\\\Manpower IT\\\\Auto-Match\\\\Candidates-to-Jobs-Auto-Match-Cortex-AI'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9155a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-09 14:35:25,448: INFO: connection: Snowflake Connector for Python Version: 3.7.0, Python Version: 3.10.11, Platform: Windows-10-10.0.26100-SP0]\n",
      "[2025-10-09 14:35:25,448: INFO: connection: This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.]\n",
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n",
      "Going to open: https://login.microsoftonline.com/e2ba81b8-03fe-407c-96a1-f4bc0f512e7d/saml2?SAMLRequest=nZJPb%2BIwEMW%2FSuQ9x3HCfwuoKKgq2rbLklCpezPJBKw6dmo7BPj06wSQuof2sDfLfjO%2F53kzvjsWwjuANlzJCQoxQR7IVGVc7iZokzz4Q%2BQZy2TGhJIwQScw6G46NqwQJZ1Vdi%2FX8FGBsZ5rJA1tHiao0pIqZrihkhVgqE1pPHt%2BohEmlBkD2jocupZkhjvW3tqSBkFd17juYKV3QUQICcgocKpG8gN9QpTfM0qtrEqVuJUc3Z%2B%2BQIQB6TYIp3CE1bXwnsvLCL6jbC8iQx%2BTZOWvfsUJ8ma3382VNFUBOgZ94Cls1k8XA8Y5uI9HXRINce3m5kOlVQmYnSsN2EhV54K9Q6qKsrKuO3anIIcsEGrH3cyWiwkq33l2WLz9PLHVm4g%2FDufF73D9uN51NpYN5jY5x%2FN975io7enw%2BqyWKfJebwlHTcJLYypYyiZX665I1PND4pNREka006NRH3e6%2FT%2FIWzh%2FXDLbVt7Mtz5wwVOtjMqtkoJLaF1CtGXDcDv0SScHv0sGqT%2Fqs9DPu9uU5L0wgkEWNOlF6LJBtDWip%2F87l3Hwuct1KV9cTsvFSgmenrwHpQtmv44xxGF7wzM%2Fb6UUCsbFLMs0GOPiFELVcw3Mut23ugIUTC%2FUf7d%2F%2Bhc%3D&RelayState=ver%3A1-hint%3A126482533224670-ETMsDgAAAZnI%2BBgKABRBRVMvQ0JDL1BLQ1M1UGFkZGluZwEAABAAEKKHiRiXu1aFQKITYQvN9pQAAACggywMiMIyRxQKgfel%2FpzIvGtxxedUiZqbedRYDnp8LOD7NRkmdbkwiXzujaK%2B8gyNOU80FYL7veFLBXgz3V3Rc%2BKVVaqm9loUqHQYOYUKAvmXb7RwxDhAN4wPkx%2FpCGQKfDnBN%2BqVXzMHg0%2BKkTmJB7g062VMN8MTnAmrRKEfqJTIXk%2Bs1QuAqDpZjE2eriGhkLw94Ewxerd9gmldZT%2B17AAUS1EpbTLxZk%2Fdt8q5OF%2B095AQKiM%3D to authenticate...\n",
      "[2025-10-09 14:35:30,620: INFO: session: Snowpark Session information: \n",
      "\"version\" : 1.15.0,\n",
      "\"python.version\" : 3.10.11,\n",
      "\"python.connector.version\" : 3.7.0,\n",
      "\"python.connector.session.id\" : 126482779930574,\n",
      "\"os.name\" : Windows\n",
      "]\n",
      "[2025-10-09 14:35:30,620: INFO: cursor: query: [use database IT_DISCOVERY]]\n",
      "[2025-10-09 14:35:30,819: INFO: cursor: query execution done]\n",
      "[2025-10-09 14:35:30,820: INFO: cursor: Number of results in first chunk: 1]\n",
      "[2025-10-09 14:35:30,820: INFO: cursor: query: [use schema CONSUMER_INT_MODEL]]\n",
      "[2025-10-09 14:35:31,035: INFO: cursor: query execution done]\n",
      "[2025-10-09 14:35:31,035: INFO: cursor: Number of results in first chunk: 1]\n"
     ]
    }
   ],
   "source": [
    "from autoMatch.utils.snowflake_utils import get_snowpark_session\n",
    "session = get_snowpark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc40000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: str\n",
    "    database: str\n",
    "    schema: str\n",
    "    input_table: str\n",
    "    output_table: str\n",
    "    italian_cities_file: str\n",
    "    output_table_italian_cities: str\n",
    "    columns: dict\n",
    "    start_date: str\n",
    "    end_date: str\n",
    "    italian_cities_string_columns: dict\n",
    "    italian_cities_numeric_columns: dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdecf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoMatch.constants import *\n",
    "from autoMatch.utils.common import read_yaml, create_directories\n",
    "from autoMatch import logger\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "        schema = self.schema.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            database=config.database,\n",
    "            schema=config.schema,\n",
    "            input_table=config.input_table,\n",
    "            output_table = config.output_table,\n",
    "            italian_cities_file = config.italian_cities_file,\n",
    "            output_table_italian_cities = config.output_table_italian_cities,\n",
    "            columns = schema.columns,\n",
    "            start_date = schema.date_range.start_date,\n",
    "            end_date = schema.date_range.end_date,\n",
    "            italian_cities_string_columns = schema.cities_file_columns.string_columns,\n",
    "            italian_cities_numeric_columns = schema.cities_file_columns.numeric_columns,\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af745682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def read_table(self, session):\n",
    "        \"\"\"\n",
    "        Reads input table\n",
    "        Function returns Snowflake dataframe\n",
    "        \"\"\"\n",
    "        database = self.config.database\n",
    "        schema = self.config.schema\n",
    "        input_table = self.config.input_table\n",
    "        columns = self.config.columns\n",
    "        start_date = self.config.start_date\n",
    "        end_date = self.config.end_date\n",
    "\n",
    "        df = session.table(f\"{database}.{schema}.{input_table}\")\n",
    "        df = df.select([col(c) for c in columns])\n",
    "        df = df.filter((col(\"date_added\") >= start_date) & (col(\"date_added\") <= end_date))\n",
    "        logger.info(f\"Table {input_table} successfully read. Number of rows: {df.count()}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def read_cities_file(self, session):\n",
    "        \"\"\"\n",
    "        Reads XLSX file containing italian cities\n",
    "        Function returns Snowflake dataframe\n",
    "        \"\"\"\n",
    "        italian_cities_file = self.config.italian_cities_file\n",
    "        string_columns = self.config.italian_cities_string_columns\n",
    "        numeric_columns = self.config.italian_cities_numeric_columns\n",
    "\n",
    "        df = pd.read_excel(italian_cities_file, header=0)\n",
    "\n",
    "        # Rename columns for consistency (optional but recommended)\n",
    "        df.columns = [col.strip().replace(\" \", \"_\").lower() for col in df.columns]\n",
    "        df = df[string_columns + numeric_columns]\n",
    "        \n",
    "        # Convert ZIP to string (preserve leading zeros)\n",
    "        df[\"zip\"] = df[\"zip\"].apply(lambda x: str(int(x)) if pd.notnull(x) else None)\n",
    "        \n",
    "        # Convert string columns\n",
    "        for col in string_columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "        # Convert latitude and longitude to float, handle NaNs\n",
    "        for col in numeric_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        logger.info(f\"XLSX file containing italian cities successfully read\")\n",
    "        print(df.head(3))\n",
    "        print(df.info())\n",
    "\n",
    "        return session.create_dataframe(df)\n",
    "\n",
    "    def write_table(self, df, table_name = 'output_table'):\n",
    "        \"\"\"\n",
    "        Writes table\n",
    "        Function returns nothing\n",
    "        \"\"\"\n",
    "\n",
    "        df.write.save_as_table(table_name, mode=\"overwrite\")\n",
    "        logger.info(f\"Table {table_name} successfully written\")\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4352f8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-09 14:55:39,878: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-10-09 14:55:39,881: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-10-09 14:55:39,888: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-10-09 14:55:39,890: INFO: common: created directory at: artifacts]\n",
      "[2025-10-09 14:55:39,892: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "[2025-10-09 14:55:43,382: INFO: 626174755: XLSX file containing italian cities successfully read]\n",
      "                      unique_identifier          city_name province  \\\n",
      "0  02f27cc1-ab58-4115-93b1-3cde2ff3754e  Abano Terme Bagni       PD   \n",
      "1  b38d8576-e4a6-49b1-8d52-4e2ae0559cf2         Abatemarco       SA   \n",
      "2  dd0097c0-51c2-408e-aee0-7a1c98d85428  Abazia Di Sulmona       AQ   \n",
      "\n",
      "  province_ext    zip   latitude  longitude  \n",
      "0       Padova  35031        NaN        NaN  \n",
      "1      Salerno  84040  40.144668  15.355906  \n",
      "2     L'Aquila  67030        NaN        NaN  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14481 entries, 0 to 14480\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   unique_identifier  14481 non-null  object \n",
      " 1   city_name          14481 non-null  object \n",
      " 2   province           14481 non-null  object \n",
      " 3   province_ext       14481 non-null  object \n",
      " 4   zip                14481 non-null  object \n",
      " 5   latitude           12388 non-null  float64\n",
      " 6   longitude          12388 non-null  float64\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 792.1+ KB\n",
      "None\n",
      "[2025-10-09 14:55:43,398: INFO: cursor: query: [CREATE TEMP STAGE /* Python:snowflake.connector.pandas_tools.write_pandas() */ \"...]]\n",
      "[2025-10-09 14:55:44,410: INFO: cursor: query execution done]\n",
      "[2025-10-09 14:55:44,410: INFO: cursor: Number of results in first chunk: 1]\n",
      "[2025-10-09 14:55:44,522: INFO: cursor: query: [PUT /* Python:snowflake.connector.pandas_tools.write_pandas() */ 'file://C:\\\\Use...]]\n",
      "[2025-10-09 14:55:44,747: INFO: cursor: query execution done]\n",
      "[2025-10-09 14:55:46,176: INFO: cursor: query: [CREATE TEMP FILE FORMAT \"IT_DISCOVERY\".\"CONSUMER_INT_MODEL\".\"uytandjrdy\" /* Pyth...]]\n",
      "[2025-10-09 14:55:46,405: INFO: cursor: query execution done]\n",
      "[2025-10-09 14:55:46,405: INFO: cursor: Number of results in first chunk: 1]\n",
      "[2025-10-09 14:55:46,405: INFO: cursor: query: [SELECT COLUMN_NAME, TYPE FROM table(infer_schema(location=>'@\"IT_DISCOVERY\".\"CON...]]\n",
      "[2025-10-09 14:55:47,420: INFO: cursor: query execution done]\n",
      "[2025-10-09 14:55:47,422: INFO: cursor: Number of results in first chunk: 7]\n",
      "[2025-10-09 14:55:47,424: INFO: cursor: query: [CREATE TEMPORARY TABLE IF NOT EXISTS \"IT_DISCOVERY\".\"CONSUMER_INT_MODEL\".\"SNOWPA...]]\n",
      "[2025-10-09 14:55:47,820: INFO: cursor: query execution done]\n",
      "[2025-10-09 14:55:47,821: INFO: cursor: Number of results in first chunk: 1]\n",
      "[2025-10-09 14:55:47,822: INFO: cursor: query: [COPY INTO \"IT_DISCOVERY\".\"CONSUMER_INT_MODEL\".\"SNOWPARK_TEMP_TABLE_H4O6MQTLU2\" /...]]\n",
      "[2025-10-09 14:55:49,170: INFO: cursor: query execution done]\n",
      "[2025-10-09 14:55:49,170: INFO: cursor: Number of results in first chunk: 1]\n",
      "[2025-10-09 14:55:49,174: INFO: cursor: query: [SELECT  *  FROM (\"IT_DISCOVERY\".\"CONSUMER_INT_MODEL\".\"SNOWPARK_TEMP_TABLE_H4O6MQ...]]\n",
      "[2025-10-09 14:55:49,422: INFO: cursor: query execution done]\n",
      "[2025-10-09 14:55:49,422: INFO: cursor: Number of results in first chunk: 0]\n",
      "[2025-10-09 14:55:49,422: INFO: cursor: query: [CREATE  OR  REPLACE    TABLE  MPG_IT_AUTOMATCH_ITALIAN_CITIES(\"unique_identifier...]]\n",
      "[2025-10-09 14:55:50,355: INFO: cursor: query execution done]\n",
      "[2025-10-09 14:55:50,356: INFO: cursor: Number of results in first chunk: 1]\n",
      "[2025-10-09 14:55:50,356: INFO: 626174755: Table MPG_IT_AUTOMATCH_ITALIAN_CITIES successfully written]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    df = data_ingestion.read_table(session)\n",
    "    data_ingestion.write_table(df, data_ingestion_config.output_table)\n",
    "    df = data_ingestion.read_cities_file(session)\n",
    "    data_ingestion.write_table(df, data_ingestion_config.output_table_italian_cities)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbeba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
