{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875de2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\fiscarelli\\\\Desktop\\\\Progetti\\\\Manpower IT\\\\Auto-Match\\\\Candidates-to-Jobs-Auto-Match-Cortex-AI'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "\n",
    "# Add the absolute path to src/ so Python can find automatch\n",
    "src_path = os.path.abspath(\"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c1705f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\fiscarelli\\\\Desktop\\\\Progetti\\\\Manpower IT\\\\Auto-Match\\\\Candidates-to-Jobs-Auto-Match-Cortex-AI'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9155a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-12 15:30:11,014: INFO: connection: Snowflake Connector for Python Version: 3.7.0, Python Version: 3.10.11, Platform: Windows-10-10.0.26100-SP0]\n",
      "[2025-10-12 15:30:11,016: INFO: connection: This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.]\n",
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n",
      "Going to open: https://login.microsoftonline.com/e2ba81b8-03fe-407c-96a1-f4bc0f512e7d/saml2?SAMLRequest=nZLBTuMwEIZfJfKeE9tpS1urLSpUFV0B7TYFIW6OMykWjh1shwBPv25KJfYAh71Fzjfjz%2FPP5PytUtErWCeNniKaEBSBFqaQej9Fd7tlPEKR81wXXBkNU%2FQODp3PJo5Xqmbzxj%2FpLbw04HwUGmnHuh9T1FjNDHfSMc0rcMwLls1vrlmaEFZb440wCn0p%2BbmCOwfWB8NTSeFk0HvyvmYYt22btL3E2D1OCSGYjHGgDsivE%2F8W3vQNTzHpH%2FhABHzz6XYh9XEEP2nlR8ixq91uE2%2FW2Q5F85PqpdGuqcBmYF%2BlgLvt9VHABYOLbNwn6Shpw9xiaKypIeEfjYXEadOWij%2BDMFXd%2BNA9CV%2B4hAIrs5dhAKvFFNXPssi2WX215B8Pub%2B5Xf8R97%2FnRc5XYDZkPXgYq%2B3AtLqqXwbbkUDR%2FSnh9JDwyrkGVvqQqw9HJB3ElMQ03dEe6xFGAzRMH1G0CH5Sc99VnuQ7j6SSwhpnSm%2B0kho6S0hzPqL5KCa9EuI%2BGYp4fMZpXPZzQcoBTWFY4EPaKTpuEOtE7Ox%2F5zLBX7t8LuVtyGm12BglxXu0NLbi%2FvsYaUK7E1nEZYcyqLhU86Kw4FyIUynTXlrgPuy%2Btw0gPDve%2Bu%2F2z%2F4C&RelayState=ver%3A1-hint%3A126482533224670-ETMsDgAAAZnYnU7oABRBRVMvQ0JDL1BLQ1M1UGFkZGluZwEAABAAEHMYrHBUz6nNY4keZbekcKUAAACgEsRvGJYO%2F3A8I9FHib2rZX9UQ1KBLMAEm53M%2F%2BwVDL20ncD2uHPC2ZSv48l2hCIHqPZAuvanHH697UbNk8NP8f8S4ieQcoP6c2Qxmb%2Fl3jV1SPAdISbT5nzPsVKZaj8t43hVibT77lWPlGpLdhiLgGvTnocD6MtKkOAb2SisLmhu7fb4RKRhVO7yx7fj0yDeolVLbHgrFj1vtjjrnYyx5AAUvZIWvdv4P5BZ%2BmwwS3A56Qq5314%3D to authenticate...\n",
      "[2025-10-12 15:30:14,981: INFO: session: Snowpark Session information: \n",
      "\"version\" : 1.15.0,\n",
      "\"python.version\" : 3.10.11,\n",
      "\"python.connector.version\" : 3.7.0,\n",
      "\"python.connector.session.id\" : 126482780959702,\n",
      "\"os.name\" : Windows\n",
      "]\n",
      "[2025-10-12 15:30:14,983: INFO: cursor: query: [use database IT_DISCOVERY]]\n",
      "[2025-10-12 15:30:15,095: INFO: cursor: query execution done]\n",
      "[2025-10-12 15:30:15,096: INFO: cursor: Number of results in first chunk: 1]\n",
      "[2025-10-12 15:30:15,097: INFO: cursor: query: [use schema CONSUMER_INT_MODEL]]\n",
      "[2025-10-12 15:30:15,282: INFO: cursor: query execution done]\n",
      "[2025-10-12 15:30:15,287: INFO: cursor: Number of results in first chunk: 1]\n"
     ]
    }
   ],
   "source": [
    "from autoMatch.utils.snowflake_utils import get_snowpark_session\n",
    "session = get_snowpark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc40000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: str\n",
    "    database: str\n",
    "    schema: str\n",
    "    input_table: str\n",
    "    output_table: str\n",
    "    italian_cities_file: str\n",
    "    output_table_italian_cities: str\n",
    "    columns: dict\n",
    "    start_date: str\n",
    "    end_date: str\n",
    "    italian_cities_string_columns: dict\n",
    "    italian_cities_numeric_columns: dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdecf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoMatch.constants import *\n",
    "from autoMatch.utils.common import read_yaml, create_directories\n",
    "from autoMatch import logger\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "        schema = self.schema.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            database=config.database,\n",
    "            schema=config.schema,\n",
    "            input_table=config.input_table,\n",
    "            output_table = config.output_table,\n",
    "            italian_cities_file = config.italian_cities_file,\n",
    "            output_table_italian_cities = config.output_table_italian_cities,\n",
    "            columns = schema.columns,\n",
    "            start_date = schema.date_range.start_date,\n",
    "            end_date = schema.date_range.end_date,\n",
    "            italian_cities_string_columns = schema.cities_file_columns.string_columns,\n",
    "            italian_cities_numeric_columns = schema.cities_file_columns.numeric_columns,\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af745682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col, is_null\n",
    "from snowflake.snowpark.types import StructType, StructField, StringType, FloatType\n",
    "from snowflake.snowpark import Row\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def read_table(self, session):\n",
    "        \"\"\"\n",
    "        Reads input table\n",
    "        Function returns Snowflake dataframe\n",
    "        \"\"\"\n",
    "        database = self.config.database\n",
    "        schema = self.config.schema\n",
    "        input_table = self.config.input_table\n",
    "        columns = self.config.columns\n",
    "        start_date = self.config.start_date\n",
    "        end_date = self.config.end_date\n",
    "\n",
    "        df = session.table(f\"{database}.{schema}.{input_table}\")\n",
    "        df = df.select([col(c) for c in columns])\n",
    "        df = df.filter((col(\"date_added\") >= start_date) & (col(\"date_added\") <= end_date))\n",
    "        logger.info(f\"Table {input_table} successfully read. Number of rows: {df.count()}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def read_cities_file(self, session):\n",
    "        \"\"\"\n",
    "        Reads XLSX file containing italian cities\n",
    "        Function returns Snowflake dataframe\n",
    "        \"\"\"\n",
    "        italian_cities_file = self.config.italian_cities_file\n",
    "        string_columns = self.config.italian_cities_string_columns\n",
    "        numeric_columns = self.config.italian_cities_numeric_columns\n",
    "\n",
    "        df = pd.read_excel(italian_cities_file, header=0)\n",
    "\n",
    "        # Rename columns for consistency (optional but recommended)\n",
    "        df.columns = (\n",
    "            df.columns\n",
    "            .str.strip()\n",
    "            .str.replace(\" \", \"_\")\n",
    "            .str.replace('\"', '')\n",
    "            .str.replace(\"'\", '')\n",
    "            .str.lower()\n",
    "        )\n",
    "        df = df[string_columns + numeric_columns]\n",
    "        \n",
    "        # Convert ZIP to string (preserve leading zeros)\n",
    "        df[\"zip\"] = df[\"zip\"].apply(lambda x: str(int(x)).zfill(5) if pd.notnull(x) else None)\n",
    "        \n",
    "        # Convert string columns\n",
    "        for col in string_columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "        # Convert latitude and longitude to float, handle NaNs\n",
    "        for col in numeric_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        # Remove rows with missing city_name\n",
    "        df = df[\n",
    "            df[\"city_name\"].notna() &  # Remove NaN and None\n",
    "            (df[\"city_name\"].str.strip() != \"\") &  # Remove empty and whitespace-only strings\n",
    "            (df[\"city_name\"].str.lower().str.strip() != \"null\") &  # Remove \"NULL\" string\n",
    "            (df[\"city_name\"].str.lower().str.strip() != \"nan\")  # Remove \"nan\" string\n",
    "        ]\n",
    "        \n",
    "        #These are necessary in order to avoid columns names qith quotes (e.g. \"city\" instead of city)\n",
    "        rows = [Row(**row) for row in df.to_dict(orient=\"records\")]\n",
    "        schema = StructType([\n",
    "            StructField(\"unique_identifier\", StringType()),\n",
    "            StructField(\"city_name\", StringType()),\n",
    "            StructField(\"province\", StringType()),\n",
    "            StructField(\"province_ext\", StringType()), \n",
    "            StructField(\"zip\", StringType()),\n",
    "            StructField(\"latitude\", FloatType()),\n",
    "            StructField(\"longitude\", FloatType())\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "        logger.info(f\"XLSX file containing italian cities successfully read\")\n",
    "        print(df.head(3))\n",
    "        print(df.info())\n",
    "\n",
    "        return session.create_dataframe(rows, schema=schema)\n",
    "\n",
    "    def write_table(self, df, table_name = 'output_table'):\n",
    "        \"\"\"\n",
    "        Writes table\n",
    "        Function returns nothing\n",
    "        \"\"\"\n",
    "\n",
    "        df.write.save_as_table(table_name, mode=\"overwrite\")\n",
    "        logger.info(f\"Table {table_name} successfully written\")\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4352f8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-12 15:43:56,708: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-10-12 15:43:56,711: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-10-12 15:43:56,719: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-10-12 15:43:56,721: INFO: common: created directory at: artifacts]\n",
      "[2025-10-12 15:43:56,724: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "[2025-10-12 15:43:59,675: INFO: 2056248651: XLSX file containing italian cities successfully read]\n",
      "                      unique_identifier                 city_name province  \\\n",
      "0  eac60bbe-11b5-4c58-a394-16495e22fa12                    Casape       RM   \n",
      "1  1459faed-108c-45d1-887a-fd9f5d62f4e4            Castelchiodato       RM   \n",
      "2  ec66f727-d171-4fa0-b168-201cfdada0a1  Castelverde Di Lunghezza       RM   \n",
      "\n",
      "  province_ext    zip   latitude  longitude  \n",
      "0         Roma  00010  41.906517  12.885287  \n",
      "1         Roma  00010  42.057184  12.697253  \n",
      "2         Roma  00010        NaN        NaN  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14479 entries, 0 to 14479\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   unique_identifier  14479 non-null  object \n",
      " 1   city_name          14479 non-null  object \n",
      " 2   province           14479 non-null  object \n",
      " 3   province_ext       14479 non-null  object \n",
      " 4   zip                14479 non-null  object \n",
      " 5   latitude           12387 non-null  float64\n",
      " 6   longitude          12387 non-null  float64\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 904.9+ KB\n",
      "None\n",
      "[2025-10-12 15:44:00,016: INFO: cursor: query: [SELECT $1 AS \"UNIQUE_IDENTIFIER\", $2 AS \"CITY_NAME\", $3 AS \"PROVINCE\", $4 AS \"PR...]]\n",
      "[2025-10-12 15:44:00,073: INFO: cursor: query execution done]\n",
      "[2025-10-12 15:44:00,073: INFO: cursor: Number of results in first chunk: 0]\n",
      "[2025-10-12 15:44:00,084: INFO: cursor: query: [SELECT \"UNIQUE_IDENTIFIER\", \"CITY_NAME\", \"PROVINCE\", \"PROVINCE_EXT\", \"ZIP\", \"LAT...]]\n",
      "[2025-10-12 15:44:00,140: INFO: cursor: query execution done]\n",
      "[2025-10-12 15:44:00,141: INFO: cursor: Number of results in first chunk: 0]\n",
      "[2025-10-12 15:44:00,143: INFO: cursor: query: [CREATE  OR  REPLACE  SCOPED TEMPORARY  TABLE SNOWPARK_TEMP_TABLE_GSTXAFGO0B(\"UNI...]]\n",
      "[2025-10-12 15:44:00,412: INFO: cursor: query execution done]\n",
      "[2025-10-12 15:44:00,412: INFO: cursor: Number of results in first chunk: 1]\n",
      "[2025-10-12 15:44:00,660: INFO: cursor: query: [create or replace temporary stage SYSTEMBIND file_format=(type=csv field_optiona...]]\n",
      "[2025-10-12 15:44:00,829: INFO: cursor: query execution done]\n",
      "[2025-10-12 15:44:00,829: INFO: cursor: Number of results in first chunk: 1]\n",
      "[2025-10-12 15:44:00,832: INFO: cursor: query: [PUT file://14479.csv @SYSTEMBIND/e506df8325ed493abe98663413776053]]\n",
      "[2025-10-12 15:44:01,032: INFO: cursor: query execution done]\n",
      "[2025-10-12 15:44:01,369: INFO: cursor: query: [INSERT  INTO SNOWPARK_TEMP_TABLE_GSTXAFGO0B(\"UNIQUE_IDENTIFIER\", \"CITY_NAME\", \"P...]]\n",
      "[2025-10-12 15:44:02,368: INFO: cursor: query execution done]\n",
      "[2025-10-12 15:44:02,373: INFO: cursor: query: [CREATE  OR  REPLACE    TABLE  MPG_IT_AUTOMATCH_ITALIAN_CITIES(\"UNIQUE_IDENTIFIER...]]\n",
      "[2025-10-12 15:44:03,300: INFO: cursor: query execution done]\n",
      "[2025-10-12 15:44:03,301: INFO: cursor: Number of results in first chunk: 1]\n",
      "[2025-10-12 15:44:03,304: INFO: cursor: query: [DROP  TABLE  If  EXISTS SNOWPARK_TEMP_TABLE_GSTXAFGO0B]]\n",
      "[2025-10-12 15:44:03,472: INFO: cursor: query execution done]\n",
      "[2025-10-12 15:44:03,474: INFO: cursor: Number of results in first chunk: 1]\n",
      "[2025-10-12 15:44:03,475: INFO: 2056248651: Table MPG_IT_AUTOMATCH_ITALIAN_CITIES successfully written]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    #df = data_ingestion.read_table(session)\n",
    "    #data_ingestion.write_table(df, data_ingestion_config.output_table)\n",
    "    df = data_ingestion.read_cities_file(session)\n",
    "    data_ingestion.write_table(df, data_ingestion.config.output_table_italian_cities)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fbeba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-12 16:03:34,602: INFO: cursor: query: [select * from MPG_IT_AUTOMATCH_ITALIAN_CITIES where city_name = 'Cusago']]\n",
      "[2025-10-12 16:03:35,215: INFO: cursor: query execution done]\n",
      "[2025-10-12 16:03:35,218: INFO: cursor: Number of results in first chunk: 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNIQUE_IDENTIFIER</th>\n",
       "      <th>CITY_NAME</th>\n",
       "      <th>PROVINCE</th>\n",
       "      <th>PROVINCE_EXT</th>\n",
       "      <th>ZIP</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50dedb49-0dae-484e-b8a7-cff9fc91ecf7</td>\n",
       "      <td>Cusago</td>\n",
       "      <td>MI</td>\n",
       "      <td>Milano</td>\n",
       "      <td>20090</td>\n",
       "      <td>45.449818</td>\n",
       "      <td>9.036322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      UNIQUE_IDENTIFIER CITY_NAME PROVINCE PROVINCE_EXT  \\\n",
       "0  50dedb49-0dae-484e-b8a7-cff9fc91ecf7    Cusago       MI       Milano   \n",
       "\n",
       "     ZIP   LATITUDE  LONGITUDE  \n",
       "0  20090  45.449818   9.036322  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "session.sql(\"\"\"\n",
    "            select *\n",
    "            from MPG_IT_AUTOMATCH_ITALIAN_CITIES\n",
    "            where city_name = 'Cusago'\n",
    "            \n",
    "\"\"\").to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
